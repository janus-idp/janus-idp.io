"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2023/03/06/topology","metadata":{"permalink":"/blog/2023/03/06/topology","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-03-06-topology/index.mdx","source":"@site/blog/2023-03-06-topology/index.mdx","title":"Topology plugin coming soon to a Backstage near you!","description":"Visualize your k8s workloads in a new and efficient way!","date":"2023-03-06T00:00:00.000Z","formattedDate":"March 6, 2023","tags":[{"label":"Topology","permalink":"/blog/tags/topology"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":1.32,"hasTruncateMarker":false,"authors":[{"name":"Divyanshi Gupta","title":"Plugin Contributor","url":"https://github.com/divyanshiGupta","imageURL":"https://github.com/divyanshiGupta.png","key":"divyanshiGupta"}],"frontMatter":{"title":"Topology plugin coming soon to a Backstage near you!","authors":["divyanshiGupta"],"tags":["Topology","Plugins"],"description":"Visualize your k8s workloads in a new and efficient way!"},"nextItem":{"title":"Janus Backstage Images Now Available on quay.io","permalink":"/blog/2023/03/01/janus-backstage-images-on-quay"}},"content":"The Janus community is thrilled to share details about our Backstage Topology Plugin. This powerful new tool simplifies the process of visualizing k8s workloads of your Backstage services. With this plugin, developers can get a clear and concise overview of their application\'s structure and workload status. This eliminates the stress and cognitive overload that often comes with working with Kubernetes.\\n\\n![Topology plugin coming soon!](./topology-plugin.png)\\n\\nWith the Backstage Topology Plugin, you will be able to see a graphical visualization of your backstage service\u2019s workloads and their pods statuses across clusters in real-time with the ability to filter workloads by a specific cluster. \\n\\nSo, what makes the Backstage Topology Plugin so special? For starters, it offers a range of powerful features other than providing a graphical visualization of k8s workloads and that includes providing one click access to the running application, functionality to group workloads in multiple sets, ability to connect nodes to each other to represent their relationships with each other and providing a way to look into the details of the workload and its related resources. \\n\\nAnd best of all, the Backstage Topology Plugin is incredibly easy to use. Its intuitive interface and straightforward design mean that you won\'t have to waste time figuring out how to use it or struggling with complex settings. Instead, you can focus on getting your work done quickly and efficiently.\\n\\n## Next steps\\n\\nBe on the lookout for a more in depth overview of the Backstage Topology Plugin soon!\\n\\nLearn more about other Backstage plugins in the Janus community [here](https://github.com/janus-idp/backstage-plugins) or join us on the [Janus IDP Slack](https://join.slack.com/t/janus-idp/shared_invite/zt-1pxtehxom-fCFtF9rRe3vFqUiFFeAkmg)!"},{"id":"/2023/03/01/janus-backstage-images-on-quay","metadata":{"permalink":"/blog/2023/03/01/janus-backstage-images-on-quay","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-03-01-janus-backstage-images-on-quay/index.mdx","source":"@site/blog/2023-03-01-janus-backstage-images-on-quay/index.mdx","title":"Janus Backstage Images Now Available on quay.io","description":"Learn how to consume content hosted on quay.io, and why it matters!","date":"2023-03-01T00:00:00.000Z","formattedDate":"March 1, 2023","tags":[{"label":"Quay","permalink":"/blog/tags/quay"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":3.53,"hasTruncateMarker":false,"authors":[{"name":"Andrew Block","title":"Maintainer of Janus Helm Charts & Plugin Contributor","url":"https://github.com/sabre1041","imageURL":"https://github.com/sabre1041.png","key":"sabre1041"}],"frontMatter":{"title":"Janus Backstage Images Now Available on quay.io","authors":["sabre1041"],"tags":["Quay","Plugins"],"description":"Learn how to consume content hosted on quay.io, and why it matters!"},"prevItem":{"title":"Topology plugin coming soon to a Backstage near you!","permalink":"/blog/2023/03/06/topology"},"nextItem":{"title":"Exposing your 3scale APIs through the Backstage catalog","permalink":"/blog/2023/02/28/exposing-your-3scale-apis"}},"content":"The Janus project produces container images to support many of the active initiatives within the community. These images, built on top Red Hat certified content, help provide a stable and secure base to enable use in most environments. Previously, these images were only available within the [GitHub Container Registry](https://github.com/orgs/janus-idp/packages) service associated with [janus-idp GitHub organization](https://github.com/janus-idp/). The Janus community is happy to announce that all images produced by the Janus project are now available on [quay.io](https://quay.io) within the [janus-idp organization](https://quay.io/organization/janus-idp). quay.io is a hosted registry service for storing and building container images and distributing other OCI artifacts. With this new offering, community members and consumers can take full advantage of the benefits now provided by sourcing container content from quay.io.\\n\\n## The Significance of quay.io Integration\\nYou might be wondering why serving content on quay.io is noteworthy. Let\'s expound upon several of these reasons:\\n\\n### Security First\\nSecurity is a top of mind concern these days, and steps should be taken to ensure that all phases of development and deployment use a secure-first mentality. It was already described how the Janus Project images use certified Red Hat container images, specifically those based on the [Universal Base Image (UBI)](https://catalog.redhat.com/software/base-images). These freely available images contain the same enterprise grade packages and content as found in Red Hat Enterprise Linux (RHEL), so security and lifecycle management is top of mind.\\n\\nAnother security feature provided out-of-the-box when making use of quay.io as a container registry: image scanning. Each and every image that is published to quay.io undergoes a scan from [Clair](https://quay.github.io/clair/) to determine if any vulnerabilities are present within the image. Having an understanding of whether the image contains any current security concerns is important for both producers and consumers. Producers need to be able to determine whether the content they are producing contains any vulnerabilities and mitigate them appropriately. Consumers, more importantly, seek the ability to understand if the content they are leveraging includes any risks. This is crucial information to have at one\'s fingertips, as up to half of the content in some publicly hosted registries contain at least one critical vulnerability ([Reference](https://www.infoq.com/news/2020/12/dockerhub-image-vulnerabilities/)). With the Janus images hosted on quay.io, these benefits are now available.\\n\\n### Support Within Enterprise Environments\\nBackstage along with the concepts preached by Internal Development Platforms are seeing adoption within many types of environments, including those with enterprise concerns. While every organization is unique, there are some common traits that they share - one of which is leveraging content from trusted sources. Many of these same organizations forbid accessing external resources and operate in a fully disconnected mode. For those that use externally sourced content, steps are typically put in place to enable and allow access to these assets. \\n\\nOpenShift, Red Hat\'s Kubernetes distribution, serves platform container images from quay.io. Given that any necessary approval to access external content may have been already completed to use quay.io as a source of content, no additional steps would be needed. Otherwise, adding another namespace (quay.io/janus-idp for example) as an allowed content source may be easier to have approved since other namespaces within the same registry as there is already an existing precedent in place.\\n\\n### Continued Investment of Quay as a Container Registry\\nHosting assets within quay.io is another example of the Janus Project supporting the Quay ecosystem. Content stored in Quay (either the hosted quay.io or self managed standalone Product Red Hat Quay) can be visualized thanks to the [Quay Backstage plugin](https://github.com/janus-idp/backstage-plugins/tree/main/plugins/quay) providing many of the same data points, including security related data, all available within the Backstage dashboard. A full overview of the Quay Backstage plugin and its features can be found in [this article](../2023-02-20-exploring-quay-registry-in-backstage/index.mdx). The Quay Backstage plugin is just one of many plugins developed by the Janus community and can be found in the [backstage-plugins repository](https://github.com/janus-idp/backstage-plugins) within the [janus-idp GitHub organization](https://github.com/janus-idp/).\\n\\nSimplifying the experience surrounding the use of an Internal Development Platform is one of the core tenets of the Janus Project, and one way to stay true to this mission is making content more readily accessible and as feature rich as possible. By serving Janus Project related OCI assets within quay.io, project contributors, community members, and consumers can take advantage of this globally hosted service and all of the features it provides."},{"id":"/2023/02/28/exposing-your-3scale-apis","metadata":{"permalink":"/blog/2023/02/28/exposing-your-3scale-apis","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-02-28-exposing-your-3scale-apis/index.mdx","source":"@site/blog/2023-02-28-exposing-your-3scale-apis/index.mdx","title":"Exposing your 3scale APIs through the Backstage catalog","description":"Exposing your 3scale APIs through the Backstage catalog","date":"2023-02-28T00:00:00.000Z","formattedDate":"February 28, 2023","tags":[{"label":"3scale","permalink":"/blog/tags/3-scale"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":1.77,"hasTruncateMarker":false,"authors":[{"name":"Francisco Meneses","title":"Plugin Contributor","url":"https://github.com/fmenesesg","imageURL":"https://github.com/fmenesesg.png","key":"fmenesesg"}],"frontMatter":{"title":"Exposing your 3scale APIs through the Backstage catalog","authors":["fmenesesg"],"tags":["3scale","Plugins"],"description":"Exposing your 3scale APIs through the Backstage catalog"},"prevItem":{"title":"Janus Backstage Images Now Available on quay.io","permalink":"/blog/2023/03/01/janus-backstage-images-on-quay"},"nextItem":{"title":"Exploring Quay registry in Backstage","permalink":"/blog/2023/02/20/exploring-quay-registry-in-backstage"}},"content":"Backstage has many features that come out of the box; one of which is the API catalog. The API catalog is responsible for displaying [API entities](https://backstage.io/docs/features/software-catalog/system-model#api), which are defined in YAML format and can be stored on a git repository and used as a source to register API entities in the catalog.\\n\\nBut, what happens when you already have an API Manager like [3scale](https://www.redhat.com/en/technologies/jboss-middleware/3scale) that handles your API definitions? To better integrate 3scale and Backstage, the Janus community developed a [3scale backend plugin](https://github.com/janus-idp/backstage-plugins/tree/main/plugins/3scale-backend) that imports APIs from 3scale into the Backstage catalog as API entities.\\n\\n## Installation\\n\\nWith this plugin, your APIs from multiple 3scale tenants will be available in the Backstage catalog API entities.\\n\\nThe first step is to install the backend plugin. Navigate to the root directory of your Backstage instance and run the following command to add the plugin.\\n\\n```bash\\nyarn workspace backend add @janus-idp/backstage-plugin-3scale-backend\\n```\\n\\n### Configuration\\n\\nThe 3scale Backstage plugin allows configuration of one or many providers using the `app-config.yaml` configuration file. Use a `threeScaleApiEntity` marker to start configuring them:\\n\\n```yaml filename=\\"app-config.yaml\\" {2-11}\\ncatalog:\\n  providers:\\n    threeScaleApiEntity:\\n      dev:\\n        baseUrl: https://<TENANT>-admin.3scale.net\\n        accessToken: <ACCESS_TOKEN>\\n        schedule: # optional; same options as in TaskScheduleDefinition\\n          # supports cron, ISO duration, \\"human duration\\" as used in code\\n          frequency: { minutes: 1 }\\n          # supports ISO duration, \\"human duration\\" as used in code\\n          timeout: { minutes: 1 }\\n```\\n\\nAdd the 3scale entity provider to the catalog builder at `packages/backend/src/plugins/catalog.ts`, once done, the catalog plugin should be able to load 3scale products as entities in the Backstage catalog:\\n\\n```tsx filename=\\"packages/backend/src/plugins/catalog.ts\\" {1,7-12}\\nimport { ThreeScaleApiEntityProvider } from \'@janus-idp/backstage-plugin-3scale-backend\';\\n\\n/ ..\\n  const builder = await CatalogBuilder.create(env);\\n\\n  /** ... other processors and/or providers ... */\\n  builder.addEntityProvider(\\n    ThreeScaleApiEntityProvider.fromConfig(env.config, {\\n      logger: env.logger,\\n      scheduler: env.scheduler\\n    }),\\n  );\\n\\n  const { processingEngine, router } = await builder.build();\\n/ ..\\n```\\n\\n### Verify\\n\\nNow your API entities will be available in the Backstage catalog.\\n\\n![API entities in the Backstage catalog](./3scale.png)\\n\\n## Next steps\\n\\nTo contribute to this plugin, report issues, or provide feedback, visit our [GitHub repository](https://github.com/janus-idp/backstage-plugins/tree/main/plugins/3scale-backend) or contact us on the [Janus IDP Slack](https://join.slack.com/t/janus-idp/shared_invite/zt-1pxtehxom-fCFtF9rRe3vFqUiFFeAkmg).  Learn more about other Backstage plugins in the Janus community [here](https://github.com/janus-idp/backstage-plugins)."},{"id":"/2023/02/20/exploring-quay-registry-in-backstage","metadata":{"permalink":"/blog/2023/02/20/exploring-quay-registry-in-backstage","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-02-20-exploring-quay-registry-in-backstage/index.mdx","source":"@site/blog/2023-02-20-exploring-quay-registry-in-backstage/index.mdx","title":"Exploring Quay registry in Backstage","description":"The Janus IDP family of Backstage plugins is expanding! Please welcome our new member - a frontend plugin that enriches application view in Backstage with insights from a Quay hosted registry.","date":"2023-02-20T00:00:00.000Z","formattedDate":"February 20, 2023","tags":[{"label":"Quay","permalink":"/blog/tags/quay"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":3.73,"hasTruncateMarker":false,"authors":[{"name":"Tom Coufal","title":"Maintainer of Janus Helm Charts & Plugins","url":"https://github.com/tumido","imageURL":"https://github.com/tumido.png","key":"tumido"}],"frontMatter":{"title":"Exploring Quay registry in Backstage","authors":["tumido"],"tags":["Quay","Plugins"],"description":"The Janus IDP family of Backstage plugins is expanding! Please welcome our new member - a frontend plugin that enriches application view in Backstage with insights from a Quay hosted registry."},"prevItem":{"title":"Exposing your 3scale APIs through the Backstage catalog","permalink":"/blog/2023/02/28/exposing-your-3scale-apis"},"nextItem":{"title":"Using OpenShift Authentication to Secure Access to Backstage","permalink":"/blog/2023/02/20/using-openshift-authentication-to-secure-access-to-backstage"}},"content":"The Janus IDP family of Backstage plugins is expanding! Please welcome our new member - a frontend plugin that enriches application view in Backstage with insights from a Quay hosted registry.\\n\\nBackstage models the software ecosystem as [Backstage Catalog](https://backstage.io/docs/features/software-catalog/software-catalog-overview) entities. Users compose small individual service components into a bigger picture. And in order to truly understand and fully describe the individual building blocks, Backstage users construct views to capture different aspects of these components; from technical documentation, through dependencies and relation to deployment state, CI and CD. And part of this picture is understanding what is actually being deployed into live environments. In many cases, the deployment artifact is a container image and the desire to view all of the available and deployed container images. This new Quay plugin for Backstage brings in that capability.\\n\\nQuay is an OCI-compatible registry that allows users to build, store and distribute container images and other OCI artifacts. It is available as a hosted service on quay.io as well as a self-hosted environment deployable to any OpenShift cluster through a Quay operator. \\n\\n\\n## Installation and setup\\n\\nWith this plugin, viewing available container images in a particular repository is easy. The following guide will elaborate in detail on individual steps for enabling the integration and all currently supported settings and options.\\n\\nFirst, it is necessary to install the plugin. Please add it to the frontend of your Backstage instance:\\n\\n```bash\\nyarn workspace app add @janus-idp/backstage-plugin-quay\\n```\\n\\n### Connecting to Quay registry via Proxy\\n\\nThe plugin leverages the Backstage native proxy capabilities to query the Quay API; therefore, configurations need to be made to the `app-config.yaml`. In order to connect the Backstage instance to the public hosted Quay.io environment, the following configuration can be used:\\n\\n```yaml filename=\\"app-config.yaml\\"\\nproxy:\\n  \'/quay/api\':\\n    target: \'https://quay.io\'\\n    changeOrigin: true\\n    headers:\\n      X-Requested-With: \'XMLHttpRequest\'\\n```\\n\\nWhen accessing private Quay repositories, it may be necessary to extend this configuration with an `Authorization` header and Quay API token. This token can be obtained by creating a Quay Application using the [steps outlined in this documentation](https://docs.quay.io/api/#applications-and-tokens). Once a token is obtained, the header can be set by extending the `app-config.yaml` setup above with an additional header:\\n\\n```yaml {7} filename=\\"app-config.yaml\\"\\nproxy:\\n  \'/quay/api\':\\n    target: \'https://quay.io\'\\n    changeOrigin: true\\n    headers:\\n      X-Requested-With: \'XMLHttpRequest\'\\n      Authorization: \'Bearer ${QUAY_TOKEN}\'\\n```\\n\\nBe aware that the `QUAY_TOKEN` is an environment variable that has to be available to the Backstage instance at runtime.\\n\\nAnother popular option is to be able to target a self-hosted Quay deployment. This can be achieved by simply changing the `target` property in the settings above with the location of the Quay instance. In addition, if the self-hosted Quay registry is also deployed with a certificate that is not in the certificate chain of trust for the Backstage instance, the `secure` option has to be unset.\\n\\n```yaml {3,8} filename=\\"app-config.yaml\\"\\nproxy:\\n  \'/quay/api\':\\n    target: \'<SELF_HOSTED_QUAY_ENDPOINT>\'\\n    changeOrigin: true\\n    headers:\\n      X-Requested-With: \'XMLHttpRequest\'\\n      Authorization: \'Bearer ${QUAY_TOKEN}\'\\n    secure: [true|false]\\n```\\n\\nMore details on available Backstage proxy settings can be found in the [upstream documentation](https://backstage.io/docs/plugins/proxying). \\n\\nThis plugin conforms to the pattern used by many other Backstage plugins that use the Backstage proxy and provides a mechanism to change the default proxy path `/quay/api` via the following `app-config.yaml` settings, if needed:\\n\\n```yaml filename=\\"app-config.yaml\\"\\nquay:\\n  proxyPath: /custom/quay/path\\n```\\n\\n### Enabling Quay plugin widget in UI\\n\\nNow that the plugin is configured to access the desired Quay registry, enable the UI by enabling an additional view in the frontend application.(`packages/app/src/components/catalog/EntityPage.tsx` file in the bootstrap application)\\n\\n```tsx filename=\\"packages/app/src/components/catalog/EntityPage.tsx\\"\\nimport { QuayPage, isQuayAvailable } from \'@janus-idp/backstage-plugin-quay\';\\n\\nconst serviceEntityPage = (\\n  <EntityPageLayout>\\n    // ...\\n    <EntityLayout.Route if={isQuayAvailable} path=\\"/quay\\" title=\\"Quay\\">\\n      <QuayPage />\\n    </EntityLayout.Route>\\n  </EntityPageLayout>\\n);\\n```\\n\\n## Using the plugin\\n\\nFinally, after the plugin is fully set up, it needs to be instructed on what data to display for individual catalog entities. Extend the entity with an annotation, a similar experience Backstage users are used to with other plugins:\\n\\n```yaml\\nmetadata:\\n  annotations:\\n    \'quay.io/repository-slug\': \'<ORGANIZATION>/<REPOSITORY>\'\\n```\\n\\nFor example, if we annotate a `Component` with `\'quay.io/repository-slug\': \'janus-idp/redhat-backstage-build\'`, we are presented with the following page:\\n\\n\\n![Backstage view for janus-idp/redhat-backstage-build Quay repository](./widget.png)\\n\\n## Next steps\\n\\nAlthough this plugin doesn\u2019t have the vast features available in the Quay UI, it brings much-needed value to Backstage users.  In the future, we plan to iterate on this plugin and provide users with more insights into unique Quay functions like vulnerability scanning and detailed manifest views.\\n\\nTo contribute to this plugin, report issues, seek guidance or provide feedback, please see our GitHub repository [https://github.com/janus-idp/backstage-plugins/tree/main/plugins/quay](https://github.com/janus-idp/backstage-plugins/tree/main/plugins/quay) and/or reach out to us on [Janus IDP Slack](https://join.slack.com/t/janus-idp/shared_invite/zt-1nii16o6e-SGscZ4YtAktL6rRtZZBUfA)."},{"id":"/2023/02/20/using-openshift-authentication-to-secure-access-to-backstage","metadata":{"permalink":"/blog/2023/02/20/using-openshift-authentication-to-secure-access-to-backstage","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-02-20-using-openshift-authentication-to-secure-access-to-backstage/index.mdx","source":"@site/blog/2023-02-20-using-openshift-authentication-to-secure-access-to-backstage/index.mdx","title":"Using OpenShift Authentication to Secure Access to Backstage","description":"Using OpenShift Authentication to Secure Access to Backstage","date":"2023-02-20T00:00:00.000Z","formattedDate":"February 20, 2023","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"}],"readingTime":7.395,"hasTruncateMarker":false,"authors":[{"name":"Andrew Block","title":"Maintainer of Janus Helm Charts & Plugin Contributor","url":"https://github.com/sabre1041","imageURL":"https://github.com/sabre1041.png","key":"sabre1041"}],"frontMatter":{"title":"Using OpenShift Authentication to Secure Access to Backstage","authors":["sabre1041"],"tags":["Authentication"],"description":"Using OpenShift Authentication to Secure Access to Backstage"},"prevItem":{"title":"Exploring Quay registry in Backstage","permalink":"/blog/2023/02/20/exploring-quay-registry-in-backstage"},"nextItem":{"title":"Deploying Backstage onto OpenShift Using the Backstage Helm Chart","permalink":"/blog/2023/02/17/deploying-backstage-onto-openshift-using-helm"}},"content":"Backstage is a tool for building Internal Development Platforms and includes the flexibility to be deployed within a variety of operating environments, including containers. In prior articles, it was introduced how the deployment of Backstage within Kubernetes environments can be streamlined through the use of the [Backstage Helm chart](https://github.com/backstage/charts/tree/main/charts/backstage) as well as integrating the platform into identity stores, such as Keycloak. OpenShift is a common deployment target for running Backstage as it provides enterprise grade container orchestration and one of its capabilities is the  ability to integrate with a variety of Identity Providers. Since many users of Backstage also use OpenShift to build and deploy containerized workloads, there is a desire to more closely tie into existing workflows and access models. This article introduces how Backstage can be integrated with OpenShift\u2019s existing authentication capabilities to provide a seamless path for users to access the Backstage portal.\\n\\nIn addition to the base platform, OpenShift also includes a number of additional features that enhance not only how administrators, but developers work with the platform and include those that support GitOps and multicluster workflows just to name a few. Many of these same components also provide their own user interface which expose their capabilities. To enable ease of use, access is granted using the same authentication mechanisms as the OpenShift cluster itself using a workflow similar to the native OpenShift console. The ability to authenticate to applications using the same set of credentials as OpenShift itself is facilitated through the [OpenShift oauth-proxy](https://github.com/openshift/oauth-proxy) -- a separate container running within the same pod, using the sidecar pattern, which intercepts requests and interacts with the OpenShift platform to restrict access to facilitate the authentication process.\\n\\n## Enabling OpenShift Authentication\\n\\nAs described in the article, [Enabling Keycloak Authentication in Backstage](blog/2023-01-17-enabling-keycloak-authentication-in-backstage/index.mdx), support is available within Backstage to utilize an OAuth proxy container to restrict access to the platform. The same approach can be used for OpenShift as well. The key difference is that instead of using a [generic oauth2-proxy](https://github.com/oauth2-proxy/oauth2-proxy) container, the OpenShift oauth-proxy container is used as it has features designed specifically for OpenShift. Thanks to the versatility of the Backstage Helm chart, the other changes that need to be made to enable this feature is to the content of the Values.\\n\\nGenerally, the use of the OpenShift OAuth proxy requires the following to be configured:\\n\\n1. An OAuth client to communicate with OpenShift\\n2. TLS certificates to expose the OAuth container\\n3. Configure the Route that is exposed by the OpenShift ingress router\\n\\nLet\u2019s describe how to enable each of these configurations.\\n\\n### OAuth Client Configuration\\n\\nOpenShift provides two methods for registering a new OAuth client:\\n\\n1. Creating a OAuthClient Custom Resource\\n2. Using a Service Account as an OAuth client\\n\\nThe former is commonly used by cluster services, such as the OpenShift web console, but has the disadvantage of a cluster scoped resource, thus requiring elevated access. The latter is more applicable in this use case as a standard OpenShift Service Account can perform the function of acting as an OAuth client. All that is required to enable such functionality is adding an annotation with the key `serviceaccounts.openshift.io/oauth-redirecturi.<name>` to the Service Account with the location of the redirect URI. For the case of the OpenShift OAuth proxy, it would be at the `/oauth/callback` context path.\\n\\nThis capability can be enabled by setting the following Helm values.\\n\\n```yaml\\nserviceAccount:\\n  create: true\\n  annotations:\\n    serviceaccounts.openshift.io/oauth-redirecturi.backstage: \\"https://{{ .Values.ingress.host }}/oauth/callback\\"\\n```\\n\\n### TLS Certificate Management\\n\\nThere are multiple methods for which TLS certificates can be configured within OpenShift. They could be provided by the end user or by way of an operator, such as [cert-manager](https://docs.openshift.com/container-platform/4.12/security/cert_manager_operator/cert-manager-operator-install.html), which can integrate with an external certificate management tool or generate certificates of its own. OpenShift also includes support for automatically generating and injecting certificates into applications through the [Service Serving Certificate](https://docs.openshift.com/container-platform/4.12/security/certificates/service-serving-certificate.html) feature. The `service-ca` monitors annotations placed upon OpenShift `Service` resources. When a Service with the annotation `service.beta.openshift.io/serving-cert-secret-name=&lt;name>` is created, the controller generates a certificate and its associated private key within a `Secret`.\\n\\nTo have OpenShift generate certificate within a Secret called `backstage-tls` which can be configured within the OAuth proxy, the following Helm values can be specified:\\n\\n```yaml\\nservice:\\n  annotations:\\n    service.alpha.openshift.io/serving-cert-secret-name: backstage-tls\\n```\\n\\n### Ingress Configuration\\n\\nThe ability to expose applications running within OpenShift externally has been one of the most compelling features included within OpenShift ever since the early days of version 3. `Routes`, the predecessor to the native Kubernetes `Ingress` resource, has enabled such functionality and it continues to this day. However, more and more applications are favoring the native Kubernetes `Ingress` option over the OpenShift specific `Route`.\\n\\nFortunately, OpenShift can automatically \\"upconvert\\" Kubernetes native `Ingress` resources to OpenShift `Routes`. In addition, the upconversion can be customized. In particular, the TLS termination type to configure end-to-end secure transport. As described in the prior section, the OAuth proxy is secured using a certificate provided by the Service Serving Certificate feature. While this certificate is generated by a Certificate Authority that is trusted within OpenShift, end users would not be able to trust such certificates.\\n\\nTo work around this challenge, the generated `Route` that is created from an `Ingress` resource can be configured with a TLS termination type of `reencrypt`. TLS communication is terminated at the Ingress router and reencrypted for transport to the underlying pod. Since OpenShift assets trust the certificate that is used by the service-ca controller, trust is established for the final leg of communication enabling a fully trusted path from client to server.\\n\\nSimilar to the enablement of the Service Account as a OAuth Client and the Service Service Certificate feature, an annotation can be placed on an `Ingress` resource with the key `route.openshift.io/termination` with the value of `reencrypt` to set up the `Route` to expose the Backstage instance. \\n\\nThe following Helm values can be specified to configure the `Ingress` resource:\\n\\n```yaml\\ningress:\\n  enabled: true\\n  host: backstage.apps.example.com\\n  annotations:\\n    route.openshift.io/termination: \\"reencrypt\\"\\n```\\n\\n## Deploying the Backstage Helm Chart\\n\\nWith the primary changes called out, the final step is to declare the full set of customized Helm values and to deploy the instance of Backstage using the Backstage Helm chart.\\n\\nCreate a file called `values-backstage-openshift-auth.yaml` with the following content:\\n\\n```yaml filename=\\"values-backstage-openshift-auth.yaml\\"\\nbackstage:\\n  image:\\n    registry: quay.io\\n    repository: ablock/backstage-oauth\\n    tag: latest\\n  extraEnvVars:\\n    - name: \\"APP_CONFIG_app_baseUrl\\"\\n      value: \\"https://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_baseUrl\\"\\n      value: \\"https://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_cors_origin\\"\\n      value: \\"https://{{ .Values.ingress.host }}\\"\\n  installDir: /opt/app-root/src\\n\\n  extraContainers:\\n    - name: oauth-proxy \\n      args:\\n        - -provider=openshift\\n        - -https-address=:8888\\n        - -http-address=\\n        - -email-domain=*\\n        - -upstream=http://localhost:7007\\n        - -tls-cert=/etc/tls/private/tls.crt\\n        - -tls-key=/etc/tls/private/tls.key\\n        - -cookie-secret=\\"{{ default (randAlpha 32 | lower | b64enc) .Values.oauthProxy.cookieSecret }}\\"\\n        - -openshift-service-account={{ include \\"common.names.fullname\\" . }}\\n        - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\\n        - -skip-auth-regex=^/metrics\\n        - -skip-provider-button\\n        - -request-logging=true\\n      image: registry.redhat.io/openshift4/ose-oauth-proxy:v4.12\\n      imagePullPolicy: IfNotPresent\\n      ports:\\n        - name: oauth-proxy\\n          containerPort: 8888    \\n          protocol: TCP\\n      volumeMounts:\\n        - mountPath: /etc/tls/private\\n          name: backstage-tls\\n\\n  extraVolumeMounts:\\n    - mountPath: /tmp/fakepath\\n      name: backstage-tls\\n\\n  extraVolumes:\\n    - name: backstage-tls\\n      secret:\\n        defaultMode: 420\\n        secretName: backstage-tls\\n\\nservice:\\n  annotations:\\n    service.alpha.openshift.io/serving-cert-secret-name: backstage-tls\\n  ports:\\n    backend: 8888\\n    targetPort: oauth-proxy\\n\\nserviceAccount:\\n  create: true\\n  annotations:\\n    serviceaccounts.openshift.io/oauth-redirecturi.backstage: \\"https://{{ .Values.ingress.host }}/oauth/callback\\"\\n\\ningress:\\n  enabled: true\\n  host: backstage.apps.example.com\\n  annotations:\\n    route.openshift.io/termination: \\"reencrypt\\"\\n\\noauthProxy:\\n  cookieSecret: \\"\\"\\n```\\n\\nBe sure to update the `ingress.host` property with the desired exposed hostname for Backstage.\\n\\nThe majority of the values have been reused from prior articles on this subject aside from those that were highlighted in the previous sections as well as the `extraContainers` property which contains the definition for the oauth-proxy container. Also note that none of the customizations as it pertains to the PostgreSQL database supporting Backstage has been defined either resulting in an ephemeral deployment for demonstration purposes. Consult the prior articles, specifically [Exploring the Flexibility of the Backstage Helm Chart](blog/2023-01-25-exploring-the-flexibility-of-the-backstage-helm-chart/index.mdx), for steps on how to customize the Backstage deploying using the Helm chart along with configuring your machine with the necessary Helm dependencies.\\n\\nInstall the chart by executing the following command:\\n\\n```bash\\nhelm install -n backstage --create-namespace backstage backstage/backstage -f values-backstage-openshift-auth.yaml\\n```\\n\\nOnce the chart has been installed, open a web browser and navigate to the hostname as defined by the hostname within the Ingress resource. You should be presented with the familiar OpenShift login page for which you may be able to select the appropriate identity provider (if multiple are defined) as well as providing your credentials to complete the authentication process. \\n\\n![OpenShift login page](./openshift-login.png)\\n\\nOnce authenticated, you will be presented with the Backstage dashboard. Click the **Settings** button on the bottom left side of the page to view information related to the current authenticated user to confirm the integration with OpenShift was successful.\\n\\n![Backstage Settings - Profile identity](./backstage-profile-identity.png)\\n\\nWith minimal effort whatsoever and by modifying a few values within the Backstage Helm chart, Backstage was secured. Only those with accounts in OpenShift have the ability to access the portal. When used in conjunction with other integrations, such as the importing of organizational details from external sources, features and capabilities within Backstage can be enabled based on their access level, providing a simplified user experience that enables productivity."},{"id":"/2023/02/17/deploying-backstage-onto-openshift-using-helm","metadata":{"permalink":"/blog/2023/02/17/deploying-backstage-onto-openshift-using-helm","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-02-17-deploying-backstage-onto-openshift-using-helm/index.mdx","source":"@site/blog/2023-02-17-deploying-backstage-onto-openshift-using-helm/index.mdx","title":"Deploying Backstage onto OpenShift Using the Backstage Helm Chart","description":"Deploying Backstage onto OpenShift Using the Backstage Helm Chart","date":"2023-02-17T00:00:00.000Z","formattedDate":"February 17, 2023","tags":[{"label":"helm","permalink":"/blog/tags/helm"}],"readingTime":7.315,"hasTruncateMarker":false,"authors":[{"name":"Andrew Block","title":"Maintainer of Janus Helm Charts & Plugin Contributor","url":"https://github.com/sabre1041","imageURL":"https://github.com/sabre1041.png","key":"sabre1041"}],"frontMatter":{"title":"Deploying Backstage onto OpenShift Using the Backstage Helm Chart","authors":["sabre1041"],"tags":["helm"],"description":"Deploying Backstage onto OpenShift Using the Backstage Helm Chart"},"prevItem":{"title":"Using OpenShift Authentication to Secure Access to Backstage","permalink":"/blog/2023/02/20/using-openshift-authentication-to-secure-access-to-backstage"},"nextItem":{"title":"Ingesting Keycloak Organizational Data into the Backstage Catalog","permalink":"/blog/2023/02/06/ingesting-keycloak-org-data-into-the-backstage-catalog"}},"content":"Backstage is a framework for building developer portals and has become an important tool that is complementary with establishing an [Internal Development Platform](https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/) (IDP). Many of the same organizations who are seeking the benefits that Backstage and an IDP can provide are also using Kubernetes as a platform for building and running containerized workloads. As described within previous articles ([Part 1](/blog/2023-01-15-getting-started-with-the-backstage-helm-chart/index.mdx), [Part 2](/blog/2023-01-25-exploring-the-flexibility-of-the-backstage-helm-chart/index.mdx), the Backstage Helm chart can be used to not only simplify the process for deploying Backstage to Kubernetes, but also how its flexibility can adapt to a variety of conditions and constraints.\\n\\nWhile Kubernetes has become the de-facto container orchestration platform, there are a number of Kubernetes distributions on the market. The [Janus Project](https://janus-idp.io) is an upstream community sponsored by Red Hat and OpenShift (along with the upstream OKD) is their Kubernetes distribution. The features and capabilities that are included within OpenShift greatly benefit from a framework like Backstage as it enables end users the ability to simplify their interactions with each of these services. This article will describe the considerations that must be accounted for and the process to deploy the Backstage Helm chart to OpenShift. \\n\\n## OpenShift Environment Considerations\\n\\nAs with any target environment, there are a variety of considerations that must be accounted for in order to ensure an integration is successful -- OpenShift is no different and the following are the areas that must be addressed prior to deploying the Helm chart:\\n\\n* Image Source and Content\\n* Ingress Strategy\\n* Security\\n\\nFortunately, as described in the prior articles, the [Backstage Helm chart](https://github.com/backstage/charts/tree/main/charts/backstage) provides the necessary options to customize the deployment to suit the necessary requirements. These customizations are managed via Helm values and the following sections describe the significance of these areas as well as how they can be accomplished within the Backstage Helm chart. \\n\\n### Image Source and Content\\n\\nOpenShift encompasses an entire container platform that is built upon certified and vetted content. The majority of this content is sourced from one of the Red Hat managed container registries and include everything from the foundational platform services to content designed for application developers. Previously, it was described how the PostgreSQL instance supporting the persistence of Backstage was customized to make use of a PostgreSQL image from Software Collections instead of from Bitnami. \\n\\nUsing a similar approach, the PostgreSQL instance that is leveraged as part of the Backstage Helm chart can be configured to make use of an image from the Red Hat Container Catalog (RHCC) to provide a greater level of security and assurance. Since the official supported image from Red Hat is the downstream of the PostgreSQL from Software Collections, the only configuration that needs to be modified is the source location as shown below:\\n\\n```\\npostgresql:\\n  image:\\n    registry: registry.redhat.io\\n    repository: rhel9/postgresql-13\\n    tag: 1-73\\n```\\n\\nImages originating from Red Hat container registries can be deployed to environments other than OpenShift. However, additional configurations with regards to how to enable access to the image content needs to be applied as standard Kubernetes environments do not include the [Global Pull Secret](https://docs.openshift.com/container-platform/4.11/openshift_images/managing_images/using-image-pull-secrets.html) which include the credentials for accessing the image source. The steps for enabling this functionality is beyond the scope of this article, but the Backstage Helm chart does support this capability.\\n\\n### Ingress Strategy\\n\\nExposing applications externally for the purpose of enabling access from end users or systems is a common concern when operating in a Kubernetes environment. OpenShift saw the need for this feature from the beginning of the Kubernetes based distribution of the platform and has included a component called [`Routes`](https://docs.openshift.com/container-platform/4.9/networking/understanding-networking.html#nw-ne-comparing-ingress-route_understanding-networking) to enable this capability. Since then, the Kubernetes community has introduced a similar concept called [`Ingress`](https://kubernetes.io/docs/concepts/services-networking/ingress/) which similarly provides support for exposing applications externally.\\n\\nGiven the wide adoption of Ingress in the Kubernetes community, and to provide OpenShift users with the freedom to choose from the existing Routes approach or the more Kubernetes native Ingress feature, support was added in OpenShift to \u201cupconvert\u201d any Ingress resource that is deployed within OpenShift to an OpenShift native Route resources. This provides the best of both worlds by giving end users the flexibility to choose the approach for which they feel the most comfortable with. In addition, the up-conversion can be customized to enable Route specific features, such as specifying the TLS termination type when exposing Ingress resources in a secure fashion. The feature can be enabled by specifying the `route.openshift.io/termination` on the Ingress object itself and supports edge, passthrough and termination types.\\n\\nFor simplicity in this implementation so that TLS is offloaded at the OpenShift router, edge termination can be specified by setting the following within the Backstage Helm Values file:\\n\\n```bash\\ningress:\\n  enabled: true\\n  annotations:\\n    route.openshift.io/termination: \\"edge\\"\\n```\\n\\nBy setting this annotation, the resulting Route resource in OpenShift will be configured as a secure route with edge termination so that connections to the Backstage dashboard are secure.\\n\\n### Security\\n\\nOne of the most important aspects of OpenShift is its \u201csecure by default\u201d approach for managing the platform and all of the workloads. By default, OpenShift approaches security by enforcing that workloads conform to certain criteria including not running with elevated permissions (specifically as the root user) as well as not requesting access to privileged resources, such as file systems on each container host. This posture is inverse to a standard deployment of Kubernetes which does not require such considerations to be placed upon workloads. While this does require additional onus on those implementing and managing workloads, it does provide for a more secure operating environment.\\n\\nWhile the Backstage component of the Helm chart itself does not include any specific parameters that would require modification from a security perspective, the included [Bitnami postgres Helm chart](https://github.com/bitnami/charts/tree/main/bitnami/postgresql) does specify certain configurations that would conflict when running using OpenShift\u2019s default security profile; specifically within the `securityContext` properties of the Statefulset. Fortunately, the Bitnami postgres chart does contain options that can be used to modify the default configuration to enable a deployment into OpenShift without requiring additional configurations that would need to be employed. All that needs to be configured is to set `enabled: false` within the pod level, container level and default `securityContext` properties within the Values file as shown below\\n\\n```\\npostgresql:\\n  primary:\\n    securityContext:\\n      enabled: false\\n    podSecurityContext:\\n      enabled: false\\n    containerSecurityContext:\\n      enabled: false\\n```\\n\\n## Deploying the Backstage Helm Chart\\n\\nTaking into account each of the considerations that were discussed in the previous sections as well as the baseline configurations that need to be applied to a Fedora based container -- whether it be from the upstream Software Collections or from Red Hat\u2019s certified RHEL based images. The following is a encompassing Helm Values file that should be included in a file called `values-openshift.yaml` can be used to deploy a Red Hat based set of content (including both Backstage and PostgreSQL) in a manner that is compatible with an OpenShift environment:\\n\\n```yaml filename=\\"values-openshift.yaml\\"\\nbackstage:\\n  image:\\n    registry: ghcr.io\\n    repository: janus-idp/redhat-backstage-build\\n    tag: latest\\n  extraEnvVars:\\n    - name: \\"APP_CONFIG_app_baseUrl\\"\\n      value: \\"https://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_baseUrl\\"\\n      value: \\"https://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_cors_origin\\"\\n      value: \\"https://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_database_client\\"\\n      value: pg\\n    - name: \\"APP_CONFIG_backend_database_connection_host\\"\\n      value: \\"{{ include \\\\\\"backstage.postgresql.host\\\\\\" . }}\\"\\n    - name: \\"APP_CONFIG_backend_database_connection_port\\"\\n      value: \'5432\'\\n    - name: \\"APP_CONFIG_backend_database_connection_user\\"\\n      value: \\"{{ .Values.postgresql.auth.username }}\\"\\n    - name: \\"APP_CONFIG_backend_database_connection_password\\"\\n      valueFrom:\\n        secretKeyRef:\\n          key: postgres-password\\n          name: \\"{{ include \\\\\\"backstage.postgresql.fullname\\\\\\" . }}\\"\\n  installDir: /opt/app-root/src\\n\\ningress:\\n  enabled: true\\n  host: backstage.apps.example.com\\n  annotations:\\n    route.openshift.io/termination: \\"edge\\"\\n\\npostgresql:\\n  enabled: true\\n  database: backstage\\n  postgresqlDataDir: /var/lib/pgsql/data/userdata\\n  auth:\\n    username: postgres\\n    database: backstage\\n  image:\\n    registry: registry.redhat.io\\n    repository: rhel9/postgresql-13\\n    tag: 1-73\\n  primary:\\n    securityContext:\\n      enabled: false\\n    podSecurityContext:\\n      enabled: false\\n    containerSecurityContext:\\n      enabled: false\\n    persistence:\\n      enabled: true\\n      mountPath: /var/lib/pgsql/data\\n    extraEnvVars:\\n    - name: POSTGRESQL_ADMIN_PASSWORD\\n      valueFrom:\\n        secretKeyRef:\\n          key: postgres-password\\n          name: backstage-postgresql\\n```\\n\\nBe sure to update the `ingress.host` property with the desired hostname of the exposed Route.\\n\\nInstall the Backstage Helm chart by executing the following command that includes the location of the previously created Values file:\\n\\n```bash\\nhelm install backstage backstage/backstage -f values-openshift.yaml\\n```\\n\\n:::note\\nThe prior command assumes that the Helm CLI and the Backstage Helm repository have been added to the local machine. Consult prior articles for instructions on how to configure these steps.\\n:::\\n\\nOnce the Chart release is successful, confirm that not only that both Backstage and PostgreSQL pods are running, but that an edge terminated Route has been created to enable external access to the Backstage user interface. Open a web browser to the hostname defined within the Route to confirm the Backstage user interface can be accessed securely.\\n\\nWith only a few small steps as demonstrated within this article and thanks to the Backstage Helm chart, Backstage and its required dependencies can be deployed to an OpenShift environment. In no time at all, teams can begin building and consuming developer portals that are built on a hardened and secure foundation to enable organizations the ability to realize the benefits offered by Internal Development Platforms."},{"id":"/2023/02/06/ingesting-keycloak-org-data-into-the-backstage-catalog","metadata":{"permalink":"/blog/2023/02/06/ingesting-keycloak-org-data-into-the-backstage-catalog","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-02-06-ingesting-keycloak-org-data-into-the-backstage-catalog/index.mdx","source":"@site/blog/2023-02-06-ingesting-keycloak-org-data-into-the-backstage-catalog/index.mdx","title":"Ingesting Keycloak Organizational Data into the Backstage Catalog","description":"Ingesting Keycloak Organizational Data into the Backstage Catalog","date":"2023-02-06T00:00:00.000Z","formattedDate":"February 6, 2023","tags":[{"label":"Keycloak","permalink":"/blog/tags/keycloak"},{"label":"plugin","permalink":"/blog/tags/plugin"}],"readingTime":10.11,"hasTruncateMarker":false,"authors":[{"name":"Andrew Block","title":"Maintainer of Janus Helm Charts & Plugin Contributor","url":"https://github.com/sabre1041","imageURL":"https://github.com/sabre1041.png","key":"sabre1041"}],"frontMatter":{"title":"Ingesting Keycloak Organizational Data into the Backstage Catalog","authors":["sabre1041"],"tags":["Keycloak","plugin"],"description":"Ingesting Keycloak Organizational Data into the Backstage Catalog"},"prevItem":{"title":"Deploying Backstage onto OpenShift Using the Backstage Helm Chart","permalink":"/blog/2023/02/17/deploying-backstage-onto-openshift-using-helm"},"nextItem":{"title":"Exploring the Flexibility of the Backstage Helm Chart","permalink":"/blog/2023/01/25/exploring-the-flexibility-of-the-backstage-helm-chart"}},"content":":::note\\n  This article is a followup to the article [Enabling Keycloak Authentication in\\n  Backstage](/blog/2023-01-17-enabling-keycloak-authentication-in-backstage/index.mdx). It is important that the steps\\n  outlined within this article are completed prior as described prior to starting this article.\\n:::\\n\\nA directory service is a common component found in organizations big and small as it includes a facility for maintaining key assets including users, groups and their relationships. The Backstage catalog provides similar capabilities to assemble not only identity records, but other resources related to various software components. Items are added to the catalog manually or they are sourced from external locations. Several plugins associated with external providers including Azure, GitHub, GitLab and LDAP, support ingesting organizational data (Users and Groups) directly into the Backstage catalog.\\n\\nIn a [prior article](/blog/2023-01-17-enabling-keycloak-authentication-in-backstage/index.mdx), it was described how Keycloak can be used to act as an identity provider to store users and groups along with enforcing that users accessing Backstage authenticate against the Keycloak instance. Even though users are authenticated into Backstage, records are not added to the Backstage catalog, thus restricting the ability to fully utilize the capabilities of Backstage. Fortunately, a plugin has been developed by the Janus community to perform similar functionality as the other external providers to integrate Keycloak user and group entities into the Backstage catalog.\\n\\nThis article will describe the steps involved to implement its use within Backstage. The [keycloak-backend](https://github.com/janus-idp/backstage-plugins/tree/main/plugins/keycloak-backend) plugin is one of an increasing set of plugins found within the [backstage-plugins repository](https://github.com/janus-idp/backstage-plugins) that have been developed by the Janus community to expand the interoperability between Backstage and a variety of open source projects. These plugins are published within the [@janus-idp npm repository](https://www.npmjs.com/package/@janus-idp/backstage-plugin-keycloak-backend) which allows them to be added to Backstage with ease. Support for ingesting users and groups from Keycloak by way of the plugin only requires a few steps within Backstage itself.\\n\\n## Backstage Configuration\\n\\nThe Backstage plugin to ingest Keycloak organizational data is implemented as a [backend plugin](https://backstage.io/docs/plugins/backend-plugin). Architecturally, Backstage is separated into two separate components: the _frontend_ which includes the user interface and many other user facing features, and the _backend_ which powers a variety of plugins including the software catalog. Since the purpose of a provider (plugin) is to synchronize organization data into the Backstage catalog, it is clear to see why it is implemented as a backend plugin.\\n\\nUnlike the `oauth2Proxy` provider which was detailed in the prior article, the Keycloak backend plugin is not included as part of the standard installation of Backstage and must be installed. Plugins that are not included by default can be installed using the `yarn add` command.\\n\\nFrom the Backstage root directory, execute the following command to add the Keycloak backend plugin:\\n\\n```bash\\nyarn --cwd packages/backend add @janus-idp/backstage-plugin-keycloak-backend\\n```\\n\\nNow that the plugin has been installed, register the plugin by adding the following content to the `packages/backend/src/plugins/catalog.ts` file.\\n\\n```ts filename=\\"packages/backend/src/plugins/catalog.ts\\" {2,7-17}\\n// ..\\nimport { KeycloakOrgEntityProvider } from \'@janus-idp/backstage-plugin-keycloak-backend\';\\n\\nexport default async function createPlugin(env: PluginEnvironment): Promise<Router> {\\n  const builder = await CatalogBuilder.create(env);\\n\\n  builder.addEntityProvider(\\n    KeycloakOrgEntityProvider.fromConfig(env.config, {\\n      id: \'development\',\\n      logger: env.logger,\\n      schedule: env.scheduler.createScheduledTaskRunner({\\n        frequency: { hours: 1 },\\n        timeout: { minutes: 50 },\\n        initialDelay: { seconds: 15 },\\n      }),\\n    }),\\n  );\\n\\n  // ..\\n}\\n```\\n\\nFeel free to customize the values of the `frequency`, `timeout`, and `initialDelay` parameters as desired.\\n\\nBuild an updated container image according to the steps described [here](https://backstage.io/docs/deployment/docker) so that it can be deployed to a Kubernetes environment.\\n\\nThe Keycloak backend plugin as well as the configurations described previously are included within the reference container image is located at [quay.io/ablock/backstage-keycloak:latest](https://quay.io/repository/ablock/backstage-keycloak) if there was a desire to once again forgo producing a container image.\\n\\n## Configuring Keycloak\\n\\nEven though the majority of the configuration within Keycloak to populate Users, Groups and an OAuth client was completed previously, additional actions must be completed so that the Keycloak backend plugin has the necessary permissions to query the resources that are stored within the backstage Keycloak realm. Keycloak clients can be configured to act as a Service Account allowing for additional permissions to be granted to the client to query the Keycloak API.\\n\\nTo enable a Client to act as a Service Account, this capability, login to the Keycloak instance and navigate to the Keycloak Client created previously within the backstage realm and navigate to the **Capability config** section and check the **Service accounts roles** checkbox. Click **Save** to apply the changes.\\n\\nBy default, Keycloak Service Accounts are not granted the necessary permissions to obtain user and group information within the realm. Additional configurations are needed so that the Backstage Keycloak plugin can perform user and group queries.\\n\\n1. Login to the Keycloak instance and navigate to the backstage OAuth client within the backstage realm. Click on the Service Account roles tab so that the necessary permissions can be associated with the OAuth client.\\n\\n2. Click on the **Assign role** button to associate existing roles and enable permissions against the Keycloak Service Account.\\n\\n3. Select the **Filter by realm roles** dropdown and click **Filter by clients** to display client specific roles.\\n\\n4. Enter `realm-management` into the textbox in order to limit the number of values that are returned.\\n\\n5. Check the following roles keeping in mind that the option to select the role may only be available within a separate page:\\n\\n   - query-groups\\n   - query-users\\n   - view-users\\n\\n6. Click **Assign** to add the roles to the backstage service account. Once completed, the values present within the Service accounts role tab is represented by the screenshot below.\\n\\n![Keycloak - Service accounts roles](./keycloak-service-accounts-roles.png)\\n\\nWith the necessary Service Account roles associated with the OAuth client, the Keycloak backend plugin will be able to query the necessary information from the Keycloak API.\\n\\n## Backstage Kubernetes Deployment\\n\\nNow that both a container image of Backstage containing the necessary components to ingest Keycloak organizational data has been created and Keycloak itself has been configured to enable the Keycloak backend plugin to query the Keycloak API, the final step is to deploy an instance of Backstage to a Kubernetes environment using the [Backstage Helm chart](https://github.com/backstage/charts/tree/main/charts/backstage).\\n\\nOnce again the versatility of the Backstage Helm charts allows for a wide range of options to be configured, including the ability to enable the provider by way of environment variables within the backstage container.\\n\\nCreate a new file called `values-backstage-keycloak-plugin.yaml` containing the Helm values that will be used to enable the Keycloak backend plugin with the following content:\\n\\n```yaml filename=\\"values-backstage-keycloak-plugin.yaml\\"\\nbackstage:\\n  image:\\n    registry: quay.io\\n    repository: ablock/backstage-keycloak\\n    tag: latest\\n  extraEnvVars:\\n    - name: \'APP_CONFIG_app_baseUrl\'\\n      value: \'https://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_baseUrl\'\\n      value: \'https://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_cors_origin\'\\n      value: \'https://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_cors_origin\'\\n      value: \'https://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_catalog_providers_keycloakOrg_default_baseUrl\'\\n      value: \'{{ required \\"Keycloak BaseUrl is Required\\" .Values.keycloak.baseUrl }}\'\\n    - name: \'APP_CONFIG_catalog_providers_keycloakOrg_default_loginRealm\'\\n      value: \'{{ required \\"Keycloak Realm is Required\\" .Values.keycloak.realm }}\'\\n    - name: \'APP_CONFIG_catalog_providers_keycloakOrg_default_realm\'\\n      value: \'{{ required \\"Keycloak Realm is Required\\" .Values.keycloak.realm }}\'\\n    - name: \'APP_CONFIG_catalog_providers_keycloakOrg_default_clientId\'\\n      value: \'{{ required \\"Keycloak Client Secret is Required\\" .Values.keycloak.clientId }}\'\\n    - name: \'APP_CONFIG_catalog_providers_keycloakOrg_default_clientSecret\'\\n      value: \'{{ required \\"Keycloak Client Secret is Required\\" .Values.keycloak.clientSecret }}\'\\n\\n  extraContainers:\\n    - name: oauth2-proxy\\n      env:\\n        - name: OAUTH2_PROXY_CLIENT_ID\\n          value: \'{{ required \\"Keycloak Client Secret is Required\\" .Values.keycloak.clientId }}\'\\n        - name: OAUTH2_PROXY_CLIENT_SECRET\\n          value: \'{{ required \\"Keycloak Client Secret is Required\\" .Values.keycloak.clientSecret }}\'\\n        - name: OAUTH2_PROXY_COOKIE_SECRET\\n          value: \'{{ default (randAlpha 32 | lower | b64enc) .Values.keycloak.cookieSecret }}\'\\n        - name: OAUTH2_PROXY_OIDC_ISSUER_URL\\n          value: \'{{ required \\"Keycloak Issuer URL is Required\\" .Values.keycloak.baseUrl }}/realms/{{ required \\"Keycloak Realm is Required\\" .Values.keycloak.realm }}\'\\n        - name: OAUTH2_PROXY_SSL_INSECURE_SKIP_VERIFY\\n          value: \'true\'\\n      ports:\\n        - name: oauth-proxy\\n          containerPort: 4180\\n          protocol: TCP\\n      imagePullPolicy: IfNotPresent\\n      image: \'quay.io/oauth2-proxy/oauth2-proxy:latest\'\\n      args:\\n        - \'--provider=oidc\'\\n        - \'--email-domain=*\'\\n        - \'--upstream=http://localhost:7007\'\\n        - \'--http-address=0.0.0.0:4180\'\\n        - \'--skip-provider-button\'\\n\\nservice:\\n  ports:\\n    backend: 4180\\n    targetPort: oauth-proxy\\n\\ningress:\\n  enabled: true\\n  host: backstage.example.com\\n\\nkeycloak:\\n  baseUrl: \'<KEYCLOAK_URL>\'\\n  realm: \'backstage\'\\n  clientId: \'backstage\'\\n  clientSecret: \'\'\\n  cookieSecret: \'\'\\n```\\n\\nThe Keycloak backend plugin is enabled by declaring environment variables with the prefix `APP_CONFIG_catalog_providers_keycloakOrg_default_*` and when rendered at runtime take a form similar to the following:\\n\\n```yaml\\ncatalog:\\n  providers:\\n    keycloakOrg:\\n      default:\\n        baseUrl: <BASE_URL>\\n        loginRealm: <KEYCLOAK_LOGIN_REALM>\\n        realm: <KEYCLOAK_REALM>\\n        clientId: <KEYCLOAK_CLIENTID>\\n        clientSecret: <KEYCLOAK_CLIENTSECRET>\\n```\\n\\nSeveral fields require that the parameters be provided either within the Values file itself or as parameters using the `--set` option when deploying the chart.\\n\\nUpdate the `keycloak.baseUrl` parameter to reference the location of the Keycloak instance along with specifying the backstage OAuth client secret within the `keycloak.clientSecret` parameter. In addition, specify the hostname of the backstage instance within the `ingress.host` property. If a container image was created that includes the configurations to support not only the Keycloak backend plugin as well as OAuth integration as described in the previous article, specify the details within the `backstage.image` property.\\n\\nWith the necessary parameters configured, perform an upgrade of the Backstage helm chart by executing the following command. If an existing release does not already exist, the inclusion of the `-i` parameter ensures that it will be installed.\\n\\n```bash\\nhelm upgrade -i -n backstage --create-namespace backstage\\nbackstage/backstage -f values-backstage-keycloak-plugin.yaml\\n```\\n\\n:::note\\n  If the Backstage Helm chart was previously installed with persistence enabled using a random\\n  password generation strategy, the chart must be uninstalled first.\\n:::\\n\\nOnce the release is complete, the Backstage user interface can be accessed via the created Ingress and continues to be governed by Keycloak based OAuth authentication. However, if the log from the Backstage container is inspected, the Keycloak backend plugin can be seen in action.\\n\\nExecute the following command to view the Backstage container log:\\n\\n```bash\\nkubectl -n backstage logs deployment/backstage\\n```\\n\\n```\\n2022-12-24T23:24:36.299Z catalog info Reading Keycloak users and groups type=plugin class=KeycloakOrgEntityProvider taskId=KeycloakOrgEntityProvider:default:refresh taskInstanceId=a8c1693c-b5cb-439a-866d-c1b6b7754a77\\n2022-12-24T23:24:36.382Z catalog info Read 2 Keycloak users and 2 Keycloak groups in 0.1 seconds. Committing... type=plugin class=KeycloakOrgEntityProvider taskId=KeycloakOrgEntityProvider:default:refresh taskInstanceId=a8c1693c-b5cb-439a-866d-c1b6b7754a77\\n2022-12-24T23:24:36.386Z catalog info **Committed 2 Keycloak users and 2 Keycloak groups in 0.0 seconds.** type=plugin class=KeycloakOrgEntityProvider taskId=KeycloakOrgEntityProvider:default:refresh taskInstanceId=a8c1693c-b5cb-439a-866d-c1b6b7754a77\\n```\\n\\nObserve in the container log that the plugin identified two users and two groups from the Keycloak realm which have been imported into the backstage catalog. The contents of the Backstage catalog can be inspected by querying the Backstage API. Execute the following command to execute a command within the Backstage pod to query the API and format the results using [jq](https://stedolan.github.io/jq/). If `jq` is not installed on the local machine, it can be removed from the command.\\n\\n```bash\\nkubectl -n backstage exec -c oauth2-proxy deployment/backstage -- wget -q --output-document - \\"http://localhost:7007/api/catalog/entities?filter=kind=user\\" | jq -r\\n\\n[\\n  {\\n    \\"metadata\\": {\\n      \\"namespace\\": \\"default\\",\\n      \\"annotations\\": {\\n        \\"backstage.io/managed-by-location\\": \\"url:https://keycloak.apps.cluster-cmwgv.cmwgv.sandbox2741.opentlc.com/admin/realms/backstage/users/1e703d12-cb09-4c7e-b615-7ea620725006\\",\\n        \\"backstage.io/managed-by-origin-location\\": \\"url:https://keycloak.apps.cluster-cmwgv.cmwgv.sandbox2741.opentlc.com/admin/realms/backstage/users/1e703d12-cb09-4c7e-b615-7ea620725006\\",\\n        \\"backstage.io/view-url\\": \\"https://keycloak.apps.cluster-cmwgv.cmwgv.sandbox2741.opentlc.com/admin/realms/backstage/users/1e703d12-cb09-4c7e-b615-7ea620725006\\",\\n        \\"keycloak.org/id\\": \\"1e703d12-cb09-4c7e-b615-7ea620725006\\",\\n        \\"keycloak.org/realm\\": \\"backstage\\"\\n      },\\n      \\"name\\": \\"backstageadmin\\",\\n      \\"uid\\": \\"25f4a1bb-e035-4f3a-b618-4d16876325d7\\",\\n      \\"etag\\": \\"ab5c4076701c76d9a6215a9f7e2fd5b1e6035790\\"\\n    },\\n    \\"apiVersion\\": \\"backstage.io/v1beta1\\",\\n    \\"kind\\": \\"User\\",\\n    \\"spec\\": {\\n      \\"profile\\": {\\n        \\"email\\": \\"backstageadmin@janus-idp.io\\",\\n        \\"displayName\\": \\"Backstage Admin\\"\\n      },\\n      \\"memberOf\\": [\\n        \\"Admins\\"\\n      ]\\n    },\\n    \\"relations\\": [\\n      {\\n        \\"type\\": \\"memberOf\\",\\n        \\"targetRef\\": \\"group:default/admins\\",\\n        \\"target\\": {\\n          \\"kind\\": \\"group\\",\\n          \\"namespace\\": \\"default\\",\\n          \\"name\\": \\"admins\\"\\n        }\\n      }\\n    ]\\n  },\\n  {\\n    \\"metadata\\": {\\n      \\"namespace\\": \\"default\\",\\n      \\"annotations\\": {\\n        \\"backstage.io/managed-by-location\\": \\"url:https://keycloak.apps.cluster-cmwgv.cmwgv.sandbox2741.opentlc.com/admin/realms/backstage/users/90625bf5-5e63-434e-96b7-288908907134\\",\\n        \\"backstage.io/managed-by-origin-location\\": \\"url:https://keycloak.apps.cluster-cmwgv.cmwgv.sandbox2741.opentlc.com/admin/realms/backstage/users/90625bf5-5e63-434e-96b7-288908907134\\",\\n        \\"backstage.io/view-url\\": \\"https://keycloak.apps.cluster-cmwgv.cmwgv.sandbox2741.opentlc.com/admin/realms/backstage/users/90625bf5-5e63-434e-96b7-288908907134\\",\\n        \\"keycloak.org/id\\": \\"90625bf5-5e63-434e-96b7-288908907134\\",\\n        \\"keycloak.org/realm\\": \\"backstage\\"\\n      },\\n      \\"name\\": \\"backstageuser\\",\\n      \\"uid\\": \\"96f3f8a1-aaa2-4d4c-89dc-b3e5d22aa049\\",\\n      \\"etag\\": \\"ad2d9c10fbfad74bb685ad10fdca178b2869516c\\"\\n    },\\n    \\"apiVersion\\": \\"backstage.io/v1beta1\\",\\n    \\"kind\\": \\"User\\",\\n    \\"spec\\": {\\n      \\"profile\\": {\\n        \\"email\\": \\"backstageuser@janus-idp.io\\",\\n        \\"displayName\\": \\"Backstage User\\"\\n      },\\n      \\"memberOf\\": [\\n        \\"Users\\"\\n      ]\\n    },\\n    \\"relations\\": [\\n      {\\n        \\"type\\": \\"memberOf\\",\\n        \\"targetRef\\": \\"group:default/users\\",\\n        \\"target\\": {\\n          \\"kind\\": \\"group\\",\\n          \\"namespace\\": \\"default\\",\\n          \\"name\\": \\"users\\"\\n        }\\n      }\\n    ]\\n  }\\n]\\n```\\n\\nObserve that the relationships between users and groups are also present. Groups imported to the catalog can be inspected by executing the following command to invoke the Backstage API:\\n\\n```bash\\nkubectl -n backstage exec -c oauth2-proxy deployment/backstage -- wget -q --output-document - \\"http://localhost:7007/api/catalog/entities?filter=kind=group\\" | jq -r\\n```\\n\\nNow that the Backstage catalog has been populated, additional metadata will now be associated with users when they authenticate to the Backstage user interface. Launch a web browser and navigate to the Backstage user interface and login using either of the previously created Keycloak users.\\n\\nClick on the **Settings** button on the bottom left corner of the page. Ensure the additional relationship details (groups) are present to confirm that the authenticated user has been linked properly to the user in the catalog.\\n\\n![Backstage Settings - Profile identity](./backstage-profile-identity.png)\\n\\nThe Keycloak backend plugin will run periodically based on the parameters defined within the `catalog.ts` file to ensure that the Backstage catalog is updated with the current state as defined within keycloak. By providing the capability to ingest organizational data into the Backstage catalog from Keycloak, the benefits that are offered through the use of Keycloak as an identity source can be realized within Backstage."},{"id":"/2023/01/25/exploring-the-flexibility-of-the-backstage-helm-chart","metadata":{"permalink":"/blog/2023/01/25/exploring-the-flexibility-of-the-backstage-helm-chart","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-01-25-exploring-the-flexibility-of-the-backstage-helm-chart/index.mdx","source":"@site/blog/2023-01-25-exploring-the-flexibility-of-the-backstage-helm-chart/index.mdx","title":"Exploring the Flexibility of the Backstage Helm Chart","description":"Exploring the Flexibility of the Backstage Helm Chart","date":"2023-01-25T00:00:00.000Z","formattedDate":"January 25, 2023","tags":[{"label":"Helm","permalink":"/blog/tags/helm"}],"readingTime":10.58,"hasTruncateMarker":false,"authors":[{"name":"Andrew Block","title":"Maintainer of Janus Helm Charts & Plugin Contributor","url":"https://github.com/sabre1041","imageURL":"https://github.com/sabre1041.png","key":"sabre1041"}],"frontMatter":{"title":"Exploring the Flexibility of the Backstage Helm Chart","authors":["sabre1041"],"tags":["Helm"],"description":"Exploring the Flexibility of the Backstage Helm Chart"},"prevItem":{"title":"Ingesting Keycloak Organizational Data into the Backstage Catalog","permalink":"/blog/2023/02/06/ingesting-keycloak-org-data-into-the-backstage-catalog"},"nextItem":{"title":"Enabling Keycloak Authentication in Backstage","permalink":"/blog/2023/01/17/enabling-keycloak-authentication-in-backstage"}},"content":"Getting Backstage up and running takes no time at all thanks to the Backstage Helm chart as described in the [first article](blog/2023-01-15-getting-started-with-the-backstage-helm-chart/index.mdx) in this series. However, even though Backstage has been deployed, it is hardly ready in its current state for production use. There are several key factors that need to be addressed:\\n\\n1. Exposing Backstage properly outside the cluster\\n2. Adding persistence\\n\\n## Ingress\\n\\nAccessing the Backstage instance as part of the initial deployment made use of the `kubectl port-forward{:bash}` command, which is a key tool that is used as part of the development process. However, in order to make a deployment more representative of how Backstage would need to be configured for a production state, a proper Ingress strategy should be implemented.\\n\\nMinikube includes a set of features that extend the baseline configuration of Kubernetes, known as [addons](https://minikube.sigs.k8s.io/docs/commands/addons/). Included in the collection of minikube addons is support for deploying the [NGINX Ingress Controller](https://github.com/kubernetes/ingress-nginx) to enable more native access to resources within Kubernetes.\\n\\nExecute the following command to enable the ingress addon which will deploy the NGINX Ingress Controller into a namespace called `ingress-nginx`:\\n\\n```bash\\nminikube addons enable ingress\\n```\\n\\n### Connecting through the Minikube Ingress Controller\\n\\nAccess to resources deployed within Kubernetes through the ingress controller varies depending on the host operating system. On linux machines which can run containers natively, access can be achieved through the ip address in use by the `minikube` virtual machine. This address can be obtained by running the `minikube ip{:bash}` command.\\n\\nOn OSX machines, a tunnel can be created for which connectivity can be achieved through the ingress controller using the minikube tunnel command. Since this will expose the tunnel on ports 80 and 443, elevated permissions are needed. A password prompt will appear requesting permission to access these privileged ports.\\n\\n### Accessing Backstage through the Minikube Ingress Controller\\n\\nTo configure Backstage to expose an Ingress through the newly created ingress controller, update the content of the values that are used for the Backstage Helm chart by creating a new values file called `values-minikube-ingress.yaml` with the following content:\\n\\n```yaml filename=\\"values-minikube-ingress.yaml\\"\\nbackstage:\\n  extraEnvVars:\\n    - name: \'APP_CONFIG_app_baseUrl\'\\n      value: \'http://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_baseUrl\'\\n      value: \'http://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_cors_origin\'\\n      value: \'http://{{ .Values.ingress.host }}`\'\\n\\ningress:\\n  enabled: true\\n  host: localhost\\n```\\n\\nThe only noticeable difference between the content of these values versus the prior content contained in the `values-minikube-ingress.yaml` is enabling the creation of the Ingress resource which is triggered by setting the enabled field within the ingress property to true.\\n\\nFor those making use of a Linux machine, since the ingress controller is accessed through the IP address of the minikube VM, a fake hostname (backstage.minikube.info) can be created by setting the following value in the `/etc/hosts` file.\\n\\n```bash\\n$(minikube ip) backstage.minikube.info\\n```\\n\\nAlternatively, a wildcard IP address DNS service, such as nip.io can be used if there was a desire to avoid modifying the `/etc/hosts` file.\\n\\nRegardless of the approach, when using a Linux machine, update the value of the host field within the ingress property in the `values-minikube-ingress.yaml` file to the hostname configured above.\\n\\nCreate a new Helm release by performing an upgrade of the prior release by providing the updated values file as shown below:\\n\\n```bash\\nhelm upgrade -n backstage backstage backstage/backstage -f values-minikube-ingress.yaml\\n```\\n\\nOnce the release is complete, a new Ingress resource will have been created in the backstage namespace to expose Backstage outside the cluster.\\n\\nNavigate to the hostname that was defined in the Ingress resource. Due to caching techniques employed by many web browsers, a \u201chard reload\u201d of the page may be required in order to ensure the updated set of Javascript resources from the Backstage instance. Consult the documentation of the browser being used for the necessary steps.\\n\\nBy exposing and accessing Backstage through a Kubernetes Ingress, it better aligns to how one would want to configure Backstage for a production deployment.\\n\\n## Persistence\\n\\nTo simplify the getting started experience for users, Backstage makes use of an in-memory SQLite database as a persistent store. While this reduces the initial barrier of entry, it also limits how far one can go in their Backstage journey. Limitations with this implementation include the ability to achieve high-availability as each instance of Backstage has its own independent persistent store and existing data would be lost if the pod is restarted or deleted.\\n\\nPostgreSQL is the database backend that is used by Backstage store data persistently and the Backstage Helm chart includes the ability to provision a deployment of PostgreSQL in order to support Backstage. The [postgres Helm chart from Bitnami](https://github.com/bitnami/charts/tree/main/bitnami/postgresql) as a [dependency chart](https://helm.sh/docs/chart_best_practices/dependencies/) which is, as demonstrated previously, disabled by default. Similar to how ingress was enabled in the prior section, enabling and configuring the integration between PostgreSQL and Backstage can be achieved through the Backstage Helm chart.\\n\\nCreate a new values file called `values-minikube-persistent.yaml` with the following content:\\n\\n```yaml filename=\\"values-minikube-persistent.yaml\\"\\nbackstage:\\n  extraEnvVars:\\n    - name: \'APP_CONFIG_app_baseUrl\'\\n      value: \'http://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_baseUrl\'\\n      value: \'http://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_cors_origin\'\\n      value: \'http://{{ .Values.ingress.host }}\'\\n  args:\\n    - \'--config\'\\n    - \'/app/app-config.yaml\'\\n    - \'--config\'\\n    - \'/app/app-config.production.yaml\'\\n\\ningress:\\n  enabled: true\\n  host: localhost\\n\\npostgresql:\\n  enabled: true\\n  auth:\\n    username: bn_backstage\\n    database: backstage\\n  primary:\\n    persistence:\\n      enabled: true\\n```\\n\\nWhile no changes occurred within the set of extra environment variables, several new properties were added; specifically `args` and `postgresql`. Setting `postgresql.enabled` will trigger the installation of the postgresql chart as defined in the `Chart.yaml` file. The `postgresql` section also specifies the username and database that should be created on the PostgreSQL instance along with the creation of a PersistentVolumeClaim within Kubernetes that will serve as the backing storage for the database.\\n\\nThe `backstage.args` property is used to specify the container arguments that are injected into the Backstage container. When an installation of Backstage is created, several sample application configuration files are created: One that contains the baseline configuration for Backstage, one that features configurations that are designed for production use and one that can be used to specify configurations that are used for local development. The production configuration file (`app-config.production.yaml`) includes the necessary configurations to enable Backstage to use PostgreSQL as its persistence store. The location of these configuration files are added as arguments using the `--config` flag as declared in values file sample above.\\n\\nSeveral options are available within the Bitnami postgres chart to set the password for the newly created database use including providing it within a Kubernetes secret, explicitly as a Helm Value, or having one automatically generated which is the option that will be chosen in this case. There is one disadvantage to having the chart automatically generate the password; The `helm upgrade` command cannot be used as each invocation would result in a newly generated password, thus invalidating the prior value and causing issues for clients looking to connect.\\n\\nAs a result, uninstall the previously created chart from the minikube cluster by executing the following command:\\n\\n```bash\\nhelm uninstall -n backstage backstage\\n```\\n\\nWith the chart now uninstalled and the resources removed from the cluster, install a brand new instance of Backstage with persistence support by executing the following command. In addition, be sure to update the `host` field underneath the `ingress` property if a different value was used from the Ingress section of this article.\\n\\n```bash\\nhelm install -n backstage backstage backstage/backstage -f values-minikube-persistent.yaml\\n```\\n\\nOnce the release is complete, there will be two pods in the backstage namespace: one for the instance of backstage and the other for PostgreSQL. Confirm that the Backstage instance has connected and created the necessary tables within PostgreSQL by executing the following command:\\n\\n```bash\\nkubectl exec -it -n backstage statefulset/backstage-postgresql -- sh -c \\"export PGPASSWORD=$(kubectl get secrets -n backstage backstage-postgresql  -o jsonpath=\'{.data.postgres-password}\' | base64 -d) && psql --host localhost -U postgres -c \'\\\\l\'\\"\\n```\\n\\nSeveral tables prefixed with \u201cbackstage\u201d should be present and once confirmed, one has the assurance Backstage is making use of the PostgreSQL instance and storing data persistently.\\n\\n## Replacing the Default Images\\n\\nOne of the most common tasks for anyone working with Backstage is the customization of the components that are installed within the Backstage instance. This typically includes the addition of plugins that extend the baseline functionality of Backstage to enable the integration with external systems. The Backstage Helm chart references a container image from the Backstage community, but in many cases, does not include all of the components that may be needed by consumers. As a result, those looking to use Backstage in their own environment will need to produce their own instance of Backstage and store the resulting image in a container registry. The location of this instance can be specified as a set of Helm values to enable users to leverage their own image.\\n\\nThe Janus community also produces a minimal container image, similar to the upstream backstage community, to provide an instance of Backstage that is built from an Universal Base Image (UBI) base. Switching from the upstream Backstage image to the Janus project image can be used to demonstrate the common task of replacing where the container image of Backstage is sourced from.\\n\\nThe following values illustrate how to switch to the Janus provided image. Keep in mind that in practice, you will most likely need to use an image of your own with your specific customizations, but this provides a good example for understanding the process involved.\\n\\n```yaml\\nbackstage:\\n  image:\\n    registry: ghcr.io\\n    repository: janus-idp/redhat-backstage-build\\n    tag: latest\\n```\\n\\nAny of the Helm values files that were provided in the prior sections can be used to demonstrate substituting the location of the Backstage image.\\n\\nEach Backstage image can feature a different database schema, therefore if an existing Helm release has been deployed previously with postgresql enabled, uninstall it so that the new configurations can be applied. In addition, if persistent storage was used to support PostgreSQL, the PersistentVolumeClaim that was also created needs to be manually removed. This can be achieved using the following command:\\n\\n```bash\\nkubectl delete pvc -n backstage -l=app.kubernetes.io/name=postgresql\\n```\\n\\nOnce all of the resources have been removed, use the Backstage Helm chart to deploy Backstage with the updated set of values. Confirm the image associated with the Backstage deployment is leveraging the custom image by executing the following command:\\n\\n```bash\\nkubectl get deployment -n backstage backstage -o jsonpath=\'{ .spec.template.spec.containers[?(@.name==\\"backstage-backend\\")].image }\'\\n```\\n\\n## Customizing the PostgreSQL Configuration\\n\\nSimilar to the Backstage image itself, the image associated with the PostgreSQL instance can also be customized if there was a desire to make use of an alternate image other than the image provided by the Bitnami postgres Helm chart. Given that the Janus community is a Red Hat sponsored initiative, switching to a PostgreSQL image that is provided from [Red Hat Software Collections](https://www.redhat.com/en/resources/red-hat-software-collections) is a common task. Fortunately, the combination of features provided by the Backstage and Bitnami postgres Helm charts enable not only the customization of image location, but additional configurations that are needed to support any other required configurations needed to support an alternate image.\\n\\nCreate a new values file called `values-minikube-persistent-scl.yaml` with the following content:\\n\\n```yaml\\nbackstage:\\n  extraEnvVars:\\n    - name: \\"APP_CONFIG_app_baseUrl\\"\\n      value: \\"http://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_baseUrl\\"\\n      value: \\"http://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_cors_origin\\"\\n      value: \\"http://{{ .Values.ingress.host }}\\"\\n    - name: \\"APP_CONFIG_backend_database_connection_password\\" (1)\\n      valueFrom:\\n        secretKeyRef:\\n          key: postgres-password\\n          name: \\"{{ include \\\\\\"backstage.postgresql.fullname\\\\\\" . }}\\"\\n  args:\\n    - \\"--config\\"\\n    - \\"/app/app-config.yaml\\"\\n    - \\"--config\\"\\n    - \\"/app/app-config.production.yaml\\"\\n\\ningress:\\n  enabled: true\\n  host: localhost\\n\\npostgresql:\\n  enabled: true\\n  database: backstage\\n  postgresqlDataDir: /var/lib/pgsql/data/userdata (2)\\n  auth: (3)\\n    username: postgres\\n    database: backstage\\n  image: (4)\\n    registry: quay.io\\n    repository: fedora/postgresql-13\\n    tag: \\"13\\"\\n  primary:\\n    securityContext:\\n      enabled: false\\n    podSecurityContext:\\n      enabled: false\\n    containerSecurityContext:\\n      enabled: false\\n    persistence:\\n      enabled: true\\n      mountPath: /var/lib/pgsql/data\\n    extraEnvVars:\\n    - name: POSTGRESQL_ADMIN_PASSWORD (5)\\n      valueFrom:\\n        secretKeyRef:\\n          key: postgres-password\\n          name: backstage-postgresql\\n```\\n\\nThere are quite a number of additional configurations that are included within this values file. Let\'s break down some of the most important configurations to illustrate their significance:\\n\\n1. The Software Collections PostgreSQL image manages permissions differently than the Bitnami PostgreSQL image and the key within the generated Secret differs and needs to be provided for the Backstage container\\n2. Specifies the location of where PostgreSQL stores persistent content\\n3. The name of the database to use and user to authenticate against\\n4. The location of the Software Collections image\\n5. The environment variable used by the Software Collections PostgreSQL image to signify the password for the postgres admin account\\n\\nUninstall and reinstall the Backstage Helm chart once again so that the Software Collections image will be used to support PostgreSQL.\\n\\nAs demonstrated throughout this article, the Helm chart for Backstage provides a robust set of capabilities in order to support customizing and orchestrating a deployment to a Kubernetes environment. By simplifying the steps that it takes to deploy Backstage, the benefits when establishing an Internal Development Platform can be realized."},{"id":"/2023/01/17/enabling-keycloak-authentication-in-backstage","metadata":{"permalink":"/blog/2023/01/17/enabling-keycloak-authentication-in-backstage","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-01-17-enabling-keycloak-authentication-in-backstage/index.mdx","source":"@site/blog/2023-01-17-enabling-keycloak-authentication-in-backstage/index.mdx","title":"Enabling Keycloak Authentication in Backstage","description":"Enabling Keycloak Authentication in Backstage","date":"2023-01-17T00:00:00.000Z","formattedDate":"January 17, 2023","tags":[{"label":"keycloak","permalink":"/blog/tags/keycloak"},{"label":"authentication","permalink":"/blog/tags/authentication"}],"readingTime":11.86,"hasTruncateMarker":false,"authors":[{"name":"Andrew Block","title":"Maintainer of Janus Helm Charts & Plugin Contributor","url":"https://github.com/sabre1041","imageURL":"https://github.com/sabre1041.png","key":"sabre1041"}],"frontMatter":{"title":"Enabling Keycloak Authentication in Backstage","authors":["sabre1041"],"tags":["keycloak","authentication"],"description":"Enabling Keycloak Authentication in Backstage"},"prevItem":{"title":"Exploring the Flexibility of the Backstage Helm Chart","permalink":"/blog/2023/01/25/exploring-the-flexibility-of-the-backstage-helm-chart"},"nextItem":{"title":"Getting Started with the Backstage Helm Chart","permalink":"/blog/2023/01/15/getting-started-with-the-backstage-helm-chart"}},"content":"The software catalog is the heart of Backstage as it provides a centralized mechanism for organizing all of the assets within a particular domain. This content can include everything from services, websites, pipelines and everything in between and the catalog provides a facility for managing these assets in a declarative fashion along with assigning ownership against them. Identity records within Backstage are represented as Users (individual entities) and Groups (a collection of users) and they enable the association of ownership and policies to resources within the software catalog. The determination of who the user is and their association to a User entity within the software catalog is the core functionality of the authentication system within Backstage. Every installation of Backstage includes a number of [built-in](https://backstage.io/docs/auth/#built-in-authentication-providers) authentication providers, and while GitHub is the most common, several alternatives are available to choose from including GitLab, Google and Azure.\\n\\nKeycloak is an Open Source identity and access management tool and provides capabilities including Single Sign On (SSO), user management and support for fine grained authorization policies. In addition to these features, one of the biggest benefits of Keycloak is that it can federate identities from other external providers including many of the built-in authentication providers within Backstage. By integrating Backstage with Keycloak, a single source of truth as it relates to identity can be attained. The benefits include avoiding the process of having to manage multiple authentication providers along with allowing for a more \u201ccloud native\u201d method of authentication and authorization using the OpenID Connect (OIDC) protocol. Enabling users to authenticate against Keycloak to gain access to Backstage is a straightforward process and will be described throughout the remainder of this article.\\n\\nPrior to performing any configuration within either Keycloak or Backstage, the first step is to better understand the architecture and the overall process. Unlike other providers, such as those that were introduced previously (GitHub, Google, etc), there is no direct integration between Backstage and Keycloak. Instead, the [OAuth2 proxy provider](https://backstage.io/docs/auth/oauth2-proxy/provider) is implemented through the use of the [oauth2-proxy](https://github.com/oauth2-proxy/oauth2-proxy) to act as an intermediate for offloading the entire authentication process which passes the resulting request for Backstage to process. An overview of the entire flow is described below:\\n\\n1. OIDC client is created within Keycloak representing the integration with Backstage and configured within the OAuth2 proxy.\\n2. Users attempts to access Backstage and is redirected to Keycloak by the OAuth2 proxy\\n3. User authenticates against Keycloak\\n4. Upon successful authentication to Keycloak, OAuth process verifies user has met all necessary requirements that are needed to access Backstage\\n5. Request to Backstage for the processing of the authentication\\n6. Backstage [Sign In Resolver](https://backstage.io/docs/auth/identity-resolver) ingests request (reading headers provided by the OAuth2 proxy) and either associates the user within an existing entry in the software catalog or a new entry is created\\n7. Authentication process is complete and the user can make use of Backstage based on their level of access\\n\\nAs this list illustrates, there are several steps involved to enable Backstage users to authenticate against Keycloak. The first step is to set up Backstage with the necessary configurations to enable the OAuth2 provider.\\n\\n## Backstage Configuration\\n\\nSimilar to the other authentication providers that are included with Backstage, there are steps that must be completed within Backstage itself to support using Keycloak authentication by way of the OAuth 2 Proxy Provider including:\\n\\n- Adding the provider to the Backstage frontend\\n- Updating the Backstage app-config.yaml configuration file to enable the OAuth2 Proxy Provider\\n- Configuring a Sign in Resolver within the Backstage backend\\n\\nFirst, update the Backstage frontend by enabling the `ProxiedSignInPage` by making the following changes in the `packages/app/src/App.tsx` file:\\n\\n```tsx filename=\\"packages/app/src/App.tsx\\" {1,5-7}\\nimport { ProxiedSignInPage } from \'@backstage/core-components\';\\n\\nconst app = createApp({\\n  // ...\\n  components: {\\n    SignInPage: (props) => <ProxiedSignInPage {...props} provider=\\"oauth2Proxy\\" />,\\n  },\\n});\\n```\\n\\nNext, add the `oauth2Proxy` to the list of authentication providers within the Backstage `app-config.yaml` configuration file:\\n\\n```yaml filename=\\"app-config.yaml\\"\\nauth:\\n  providers:\\n    oauth2Proxy: {}\\n```\\n\\nThe final required configuration within backstage is to set up an [Identity Resolver](https://backstage.io/docs/auth/identity-resolver) which will translate the parameters (headers) that are received from the OAuth2 proxy and translate them into an authenticated backstage user.\\nUpdate the `packages/backend/src/plugins/auth.ts` file with the following content:\\n\\n```ts filename=\\"packages/backend/src/plugins/auth.ts\\" {1,9-40}\\nimport { DEFAULT_NAMESPACE, stringifyEntityRef } from \'@backstage/catalog-model\';\\n\\nexport default async function createPlugin(env: PluginEnvironment): Promise<Router> {\\n  return await createRouter({\\n    // ...\\n    providerFactories: {\\n      ...defaultAuthProviderFactories,\\n      // ...\\n      oauth2Proxy: providers.oauth2Proxy.create({\\n        signIn: {\\n          async resolver({ result }, ctx) {\\n            const name = result.getHeader(\'x-forwarded-preferred-username\');\\n            if (!name) {\\n              throw new Error(\'Request did not contain a user\');\\n            }\\n\\n            try {\\n              // Attempts to sign in existing user\\n              const signedInUser = await ctx.signInWithCatalogUser({\\n                entityRef: { name },\\n              });\\n\\n              return Promise.resolve(signedInUser);\\n            } catch (e) {\\n              // Create stub user\\n              const userEntityRef = stringifyEntityRef({\\n                kind: \'User\',\\n                name: name,\\n                namespace: DEFAULT_NAMESPACE,\\n              });\\n              return ctx.issueToken({\\n                claims: {\\n                  sub: userEntityRef,\\n                  ent: [userEntityRef],\\n                },\\n              });\\n            }\\n          },\\n        },\\n      }),\\n    },\\n  });\\n}\\n```\\n\\nThe logic included within the identity resolver above is as follows:\\n\\n1. Obtain the username that is provided in the x-forwarded-preferred-username by the OAuth2 proxy.\\n2. Attempt to locate the user in the Backstage catalog\\n   1. If found, sign in the user\\n3. If a user is not found, create a user on the fly and sign them in\\n\\nOnce each of the actions detailed within this section have been completed, the final step is to produce a build of Backstage. Since the target environment for this demonstration will be a Kubernetes environment, a container image will be the end result of the build process. The steps for producing a container image [can be found here](https://backstage.io/docs/deployment/docker).\\n\\nA reference container image is located at [`quay.io/ablock/backstage-keycloak:latest`](https://quay.io/repository/ablock/backstage-keycloak) if there was a desire to forgo producing a container image.\\n\\n## Configuring Keycloak\\n\\nNow that Backstage has been configured to support OAuth based authentication, the next step is to set up and configure Keycloak as an identity provider. Keycloak supports being installed in a variety of different ways including as a standalone application or within a container. Consult the [documentation](https://www.keycloak.org/guides#server) for instructions on how to get started and the process involved to install Keycloak. The easiest method, especially when deploying to a Kubernetes environment, is to use the [Keycloak Operator](https://www.keycloak.org/operator/installation). Once Keycloak has been installed and is running, launch a web browser and navigate to the web administration console and login.\\n\\nAfter authenticating to Keycloak, either create a new Realm called **backstage** or select the name of an existing Realm that will be reused.\\n\\n:::note\\n  If you choose to leverage a realm with a name other than backstage, be sure to substitute the name\\n  appropriately throughout the remainder of the article.\\n:::\\n\\nIn order to demonstrate users authenticating against Backstage, several users and groups will be created within the Realm. First select **Groups** on the left hand navigation pane and then enter the names of the two groups that should be created:\\n\\n1. Admins\\n2. Users\\n\\nOnce the groups have been provisioned, select **Users** from the left hand navigation pane and create two users with the following details:\\n\\n| Property       | User 1                      | User 2                     |\\n| -------------- | --------------------------- | -------------------------- |\\n| Username       | backstageadmin              | backstageuser              |\\n| Email          | backstageadmin@janus-idp.io | backstageuser@janus-idp.io |\\n| Email Verified | Checked                     | Checked                    |\\n| First Name     | Backstage                   | Backstage                  |\\n| Last Name      | Admin                       | User                       |\\n| Groups         | Admins                      | Users                      |\\n\\n![Create User](./create-user.png)\\n\\nAfter the accounts have been created, click the **Credentials** tab and then select **Set Password** to set an initial password for each account. Feel free to specify a password of your choosing for each user. Uncheck the **temporary** options so that a password reset is not required upon first login.\\n\\nNext, an OAuth client needs to be created that will be used by the Backstage OAuth proxy. Select the **Clients** button on the left hand navigation pane and then click **Create Client**.\\n\\nRetain the Client Type as OpenID Connect, enter **backstage** as the Client ID, and then optionally set a name and description that should be applied to the newly created client and click **Next**.\\n\\nOn the Capability Config page, ensure the Client authentication checkbox is enabled and click **Save** to create the client.\\n\\nOnly one configuration needs to be specified on the Settings tab, the Valid redirect URI\'s. This value represents the endpoint that is exposed by the OAuth2 proxy that will be sitting in front of the Backstage instance, so there is a requirement that the hostname that will be used for Backstage be known.\\n\\nThe OAuth callback url that needs to be configured in the Keycloak Valid Redirect URI\'s field takes the form `<BACKSTAGE_URL>/oauth2/callback`. So for example, if Backstage is to be accessed at https://backstage.example.com, the value that should be entered into the field would be [https://backstage.example.com/oauth2/callback](https://backstage.example.com/oauth2/callback). Once the value has been entered, click **Save**.\\n\\nThe next step is to obtain the Client Secret so that it can be used later on as part of the OAuth2-proxy configuration. Navigate to the **Credentials** page and copy the value present in the _Client Secret_ textbox.\\n\\n## Deploying Backstage using the Backstage Helm Chart\\n\\nGiven that the required prerequisites have been completed and there is a container image of Backstage available and Keycloak has been configured as an Identity Provider, the final step is to deploy Backstage. As previously mentioned, Backstage can be deployed in a variety of ways, but in this case, a deployment to a Kubernetes cluster will be used and the easiest method for deploying Backstage to Kubernetes is to use the Backstage Helm chart as it not only streamlines the deployment process, but provides the capabilities to define the required configurations to enable OAuth authentication with Keycloak. A full writeup on the [Backstage Helm chart](https://github.com/backstage/charts/tree/main/charts/backstage) including the various configurations that it enables can be found [here](blog/2023-01-15-getting-started-with-the-backstage-helm-chart/index.mdx).\\n\\nThe OAuth2 proxy that bridges the integration between Backstage and Keycloak is deployed as a sidecar container alongside Backstage. Sidecar containers can be enabled by specifying the `backstage.extraContainer` Helm Value. The entire definition of the OAuth proxy container as well as the ability to templatize the required configurations is also supported.\\n\\nCreate a new file called `values-backstage-keycloak.yaml` with the following content.\\n\\n```yaml filename=\\"values-backstage-keycloak.yaml\\"\\nbackstage:\\n  image:\\n    registry: quay.io\\n    repository: ablock/backstage-keycloak\\n    tag: latest\\n  extraEnvVars:\\n    - name: \'APP_CONFIG_app_baseUrl\'\\n      value: \'https://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_baseUrl\'\\n      value: \'https://{{ .Values.ingress.host }}\'\\n    - name: \'APP_CONFIG_backend_cors_origin\'\\n      value: \'https://{{ .Values.ingress.host }}\'\\n\\n  extraContainers:\\n    - name: oauth2-proxy\\n      env:\\n        - name: OAUTH2_PROXY_CLIENT_ID\\n          value: \'{{ required \\"Keycloak Client Secret is Required\\" .Values.keycloak.clientId }}\'\\n        - name: OAUTH2_PROXY_CLIENT_SECRET\\n          value: \'{{ required \\"Keycloak Client Secret is Required\\" .Values.keycloak.clientSecret }}\'\\n        - name: OAUTH2_PROXY_COOKIE_SECRET\\n          value: \'{{ default (randAlpha 32 | lower | b64enc) .Values.keycloak.cookieSecret }}\'\\n        - name: OAUTH2_PROXY_OIDC_ISSUER_URL\\n          value: \'{{ required \\"Keycloak Issuer URL is Required\\" .Values.keycloak.issuerUrl }}\'\\n        - name: OAUTH2_PROXY_SSL_INSECURE_SKIP_VERIFY\\n          value: \'true\'\\n      ports:\\n        - name: oauth2-proxy\\n          containerPort: 4180\\n          protocol: TCP\\n      imagePullPolicy: IfNotPresent\\n      image: \'quay.io/oauth2-proxy/oauth2-proxy:latest\'\\n      args:\\n        - \'--provider=oidc\'\\n        - \'--email-domain=*\'\\n        - \'--upstream=http://localhost:7007\'\\n        - \'--http-address=0.0.0.0:4180\'\\n        - \'--skip-provider-button\'\\n\\nservice:\\n  ports:\\n    backend: 4180\\n    targetPort: oauth2-proxy\\n\\ningress:\\n  enabled: true\\n  host: backstage.example.com\\n\\nkeycloak:\\n  issuerUrl: \'<KEYCLOAK_URL>/realms/backstage\'\\n  clientId: \'backstage\'\\n  clientSecret: \'\'\\n  cookieSecret: \'\'\\n```\\n\\n:::note\\n  The specific configurations provided within this Values file defines a minimum amount of\\n  parameters needed to enable the integration between Backstage and Keycloak. It is recommended that\\n  the configurations of the OAuth2 proxy be hardened to increase the overall level of security. See\\n  the [OAuth2 proxy\\n  documentation](https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/oauth_provider) for\\n  the full set of supported options available.\\n:::\\n\\nBefore installing the Helm chart into the Kubernetes cluster, let\u2019s review the contents of the Values file for the significance of certain parameters. The `backstage.extraContainers` parameter includes the definition of the OAuth2 Proxy and configurations are provided through a combination of container arguments and environment variables.\\n\\nThe location of the Keycloak instance is specified by providing the location of the OpenID Endpoint Configuration. This address can be identified within the Realm Settings page of the backstage Keycloak realm.\\n\\n![Realm Settings](./realm-settings.png)\\n\\nUpdate the `keycloak.issuerURL` parameter by providing the value that was obtained from the OpenID Endpoint Configuration. The `/.well-known/openid-configuration` portion of the URL can be omitted as it is inferred automatically.\\n\\nUpdate the `keycloak.clientId` and `keycloak.clientSecret` parameters with the values that were obtained from the backstage OAuth client Credentials tab previously.\\n\\nNext, specify the hostname of the backstage instance by updating the `ingress.host` parameter.\\n\\n:::note\\n  An [Ingress\\n  Controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) must be\\n  present within the cluster in order to properly serve requests destined for Backstage from sources\\n  originating outside the cluster.\\n:::\\n\\nFinally, if there was a desire to make use of a custom Backstage image that was built previously instead of the provided image, update the set of parameters underneath the `backstage.image parameter`.\\n\\nAlternatively, instead of updating the contents of the `values-backstage-keycloak.yaml` Values file, parameters can be provided during the installation of the Helm chart by each parameter using the `--set` option of the `helm install` command.\\n\\nBefore the chart can be installed, add the Backstage chart repository as well as the dependant Bitnami repository using the following commands:\\n\\n```bash\\nhelm repo add bitnami https://charts.bitnami.com/bitnami\\nhelm repo add backstage https://backstage.github.io/charts\\n```\\n\\nInstall the Backstage Helm chart to the Kubernetes cluster in a new namespace called backstage by executing the following command referencing the customized Values file:\\n\\n```bash\\nhelm install -n backstage --create-namespace backstage backstage/backstage -f values-backstage-keycloak.yaml\\n```\\n\\nOnce the Helm release is complete and the backstage container is running, open a web browser and navigate to the location of the Backstage instance.\\n\\nWhen navigating to the Backstage, the OAuth2 proxy will intercept the request and redirect the browser to the Keycloak login page.\\n\\n![Keyclock Login](./keycloak-login.png)\\n\\nLogin with either of the users that were created previously and if successful, the browser will redirect back to the Backstage user interface.\\n\\nVerify the user details have been passed from Keycloak to Backstage by clicking the **Settings** button on the left hand navigation pane.\\n\\n![Backstage Settings](./backstage-settings.png)\\n\\nNotice how the username and email address associated with the Keycloak user were passed along to Backstage for which policies and relationships can be created to customize their interactions within the portal.\\n\\nThe integration between Keycloak and Backstage enables Backstage to take advantage of the robust identity capabilities that are provided by Keycloak. By enabling users to authenticate against an instance of Keycloak, the same set of credentials can be used to access the Backstage instance and simplifies the adoption of Backstage within organizations big and small."},{"id":"/2023/01/15/getting-started-with-the-backstage-helm-chart","metadata":{"permalink":"/blog/2023/01/15/getting-started-with-the-backstage-helm-chart","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-01-15-getting-started-with-the-backstage-helm-chart/index.mdx","source":"@site/blog/2023-01-15-getting-started-with-the-backstage-helm-chart/index.mdx","title":"Getting Started with the Backstage Helm Chart","description":"Getting Started with the Backstage Helm Chart, first of a blog series","date":"2023-01-15T00:00:00.000Z","formattedDate":"January 15, 2023","tags":[{"label":"Helm","permalink":"/blog/tags/helm"}],"readingTime":4.715,"hasTruncateMarker":false,"authors":[{"name":"Andrew Block","title":"Maintainer of Janus Helm Charts & Plugin Contributor","url":"https://github.com/sabre1041","imageURL":"https://github.com/sabre1041.png","key":"sabre1041"}],"frontMatter":{"title":"Getting Started with the Backstage Helm Chart","authors":["sabre1041"],"tags":["Helm"],"description":"Getting Started with the Backstage Helm Chart, first of a blog series"},"prevItem":{"title":"Enabling Keycloak Authentication in Backstage","permalink":"/blog/2023/01/17/enabling-keycloak-authentication-in-backstage"},"nextItem":{"title":"Newly Released Backstage plugins from the Janus IDP community","permalink":"/blog/2022/12/21/newly-released-backstage-plugins"}},"content":"Red Hat [recently announced](https://developers.redhat.com/articles/2022/10/24/red-hat-joins-backstageio-community) its intention of joining the Backstage community to help shepherd the adoption of Internal Development Platforms more broadly. While there are many aspects that one needs to consider when establishing an IDP, where and how the platform will be deployed is certainly near the top of the list. Backstage can be deployed on a variety of target systems ranging from traditional infrastructure (physical servers or virtual machines) to more cloud native options. Given the popularity of Kubernetes these days, it has become a common choice for running applications and Backstage is no exception to the rule. The Janus project is Red Hat\u2019s upstream community for running Internal Development Platforms and in addition to a series of [Backstage plugins](blog/2022-12-21-newly-released-backstage-plugins/index.mdx) that have been recently developed, it has been working with the community to simplify the process for deploying Backstage on Kubernetes. Deploying an application in Kubernetes can take on many forms, and given that the Helm package manager has become the de facto standard for deploying applications on Kubernetes, the Janus project in conjunction with the greater Backstage community have come together to establish a canonical [Helm chart](https://github.com/backstage/charts) for deploying and maintaining Backstage on Kubernetes. This article will describe how easy it is to get started with the Backstage Helm chart so that an instance of Backstage can be up and running on Kubernetes in no time.\\n\\n## Installing Helm\\n\\nHelm is a versatile tool and has been integrated into a number of popular solutions as its adoption grows. However, the simplest way to demonstrate the use of the Backstage Helm chart is to utilize the standalone command line tool from a local machine. Download and install the Helm CLI from the Helm website using the method of your choosing for the target Operating System.\\n\\nOnce Helm has been installed, add the backstage Helm chart repository and its dependent repository using the following commands:\\n\\n```bash\\nhelm repo add bitnami https://charts.bitnami.com/bitnami\\nhelm repo add backstage https://backstage.github.io/charts\\n```\\n\\n:::note\\n  The Backstage Helm chart is also available as an OCI artifact. However, the steps described in\\n  this article will focus on the installation from a Helm Chart repository. Instructions on how to\\n  leverage the chart from an OCI registry can be found on the chart GitHub project repository.\\n:::\\n\\n## Deploying to Kubernetes\\n\\nSeveral options are available for accessing a Kubernetes cluster, ranging from a managed cloud provider or running one locally. Let\'s start by using Minikube, a solution for running a Kubernetes cluster locally, as the target environment for deploying the Backstage Helm chart by first installing and configuring Minikube on the local machine based on the steps described on the [Minikube website](https://minikube.sigs.k8s.io/docs/) for the target Operating System.\\n\\nOnce Minikube has been installed and configured, start an instance by executing the following command:\\n\\n```bash\\nminikube start\\n```\\n\\n:::note\\n  The Kubernetes CLI (kubectl) may be desired in order to perform commands against the minikube\\n  instance. By default, it is not installed when minikube is installed. Follow [these\\n  steps](https://minikube.sigs.k8s.io/docs/handbook/kubectl/) to configure kubectl on the local\\n  machine.\\n:::\\n\\nNow that the minikube instance is up running, the next step is to deploy the Backstage Helm chart to Kubernetes. Regardless of the operating environment that is used for Backstage, there are a few configuration details that need to be specified, particularly the baseUrl that will be used to access the platform. Backstage configuration properties can be provided in several ways and the Backstage Helm chart (thanks to both Helm\u2019s templating capabilities along with the ability to specify parameterized values) includes support for many of the most common types, including as environment variables, additional configuration files that are contained within ConfigMaps, and as inline configuration files that are transformed into ConfigMaps.\\n\\nThe most straightforward method for the purposes of this article is to define any configuration properties as environment variables which are then added as environment variables that are added to the Backstage container.\\n\\nFollowing a similar pattern as described in the documentation related to [deploying Backstage to Kubernetes](https://backstage.io/docs/deployment/k8s), create a file called values-minikube-default.yaml with the following content:\\n\\n---\\n\\n```yaml\\nbackstage:\\n  extraEnvVars:\\n    - name: \'APP_CONFIG_app_baseUrl\'\\n      value: \'http://{{ .Values.ingress.host }}:7007\'\\n    - name: \'APP_CONFIG_backend_baseUrl\'\\n      value: \'http://{{ .Values.ingress.host }}:7007\'\\n    - name: \'APP_CONFIG_backend_cors_origin\'\\n      value: \'http://{{ .Values.ingress.host }}:7007`\'\\n\\ningress:\\n  enabled: false\\n  host: localhost\\n```\\n\\nEnvironment variables with the prefix APP_CONFIG are eligible to be interpreted by Backstage as configuration properties and any field underneath the extraEnvVars property will be added to the Backstage container. The full list of how Backstage configuration properties can be defined can be found here. Also note that by default, the Backstage Helm chart creates an Ingress resource to expose Backstage outside of the Kubernetes cluster. However, minikube does not contain an Ingress controller in its default state. To access Backstage, the port-forward capability of kubectl will be used.\\n\\nDeploy Backstage to minikube by executing the following command including specifying the Values file created previously.\\n\\n```bash\\nhelm install -n backstage --create-namespace backstage backstage/backstage -f values-minikube-default.yaml\\n```\\n\\nThe preceding command deploys Backstage in a new namespace called backstage. Confirm the Backstage pod is running by executing the following command:\\n\\n```bash\\nkubectl get pods -n backstage\\n```\\n\\nNow, forward a local port to gain access to the Backstage service from the local machine:\\n\\n```bash\\nkubectl port-forward -n backstage svc/backstage 7007:7007\\n```\\n\\nOpen a web browser and navigate to [http://localhost:7007](http://localhost:7007) to view the deployed instance of Backstage.\\n\\nAnd just like that, after only a few steps, Backstage has been deployed to Kubernetes. Establishing an instance of Backstage within a Kubernetes environment is just the beginning of the journey towards achieving a robust developer platform within an organization. With the help of the Backstage Helm chart, realizing this goal becomes much more attainable."},{"id":"/2022/12/21/newly-released-backstage-plugins","metadata":{"permalink":"/blog/2022/12/21/newly-released-backstage-plugins","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-12-21-newly-released-backstage-plugins/index.mdx","source":"@site/blog/2022-12-21-newly-released-backstage-plugins/index.mdx","title":"Newly Released Backstage plugins from the Janus IDP community","description":"Keycloak and MultiCluster Engine plugins now available","date":"2022-12-21T00:00:00.000Z","formattedDate":"December 21, 2022","tags":[{"label":"Keycloak","permalink":"/blog/tags/keycloak"},{"label":"Multicluster","permalink":"/blog/tags/multicluster"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":2.465,"hasTruncateMarker":false,"authors":[{"name":"Tom Coufal","title":"Maintainer of Janus Helm Charts & Plugins","url":"https://github.com/tumido","imageURL":"https://github.com/tumido.png","key":"tumido"}],"frontMatter":{"title":"Newly Released Backstage plugins from the Janus IDP community","authors":["tumido"],"tags":["Keycloak","Multicluster","Plugins"],"date":"2022/12/21","description":"Keycloak and MultiCluster Engine plugins now available"},"prevItem":{"title":"Getting Started with the Backstage Helm Chart","permalink":"/blog/2023/01/15/getting-started-with-the-backstage-helm-chart"}},"content":"Not so long ago, [Red Hat pledged its intention](https://developers.redhat.com/articles/2022/10/24/red-hat-joins-backstageio-community) to join the Backstage community. Several weeks later we\'re starting to see the first fruits of the effort. The Janus community is pleased to announce the availability of the first 2 Backstage plugins created at Red Hat. These plugins target upstream community projects, namely [Keycloak](https://www.keycloak.org/) and [Open Cluster Management](https://open-cluster-management.io/). Both plugins are in the early stages of development and are not meant to be used yet in production environments, however [we welcome any feedback](https://github.com/janus-idp/backstage-plugins/issues) and suggestions as they continue to mature. Please join us on our path to building a better Internal Development Platforms for Kubernetes and OpenShift on the [Janus IDP community website](http://www.janus-idp.io).\\n\\nDetails related to the first Keycloak and Multicluster Engine plugins for Backstage can be found in the following sections:\\n\\n## Keycloak plugin for Backstage\\n\\nThe need for Identity management is a common concern for any internal development platform. Backstage already contains the functionality to connect to external identity providers to enable authentication and apply proper RBAC. However, these concerns are not the sole role of identity management in relation to development portals. These portals also focus on accountability, responsibility and relationship of users and contributors to their project. Backstage achieves that through entity relations within a service catalog. All actors and objects are modeled as entities in this catalog and the Keycloak plugin ensures that all of your Keycloak users and groups are properly represented and mapped within the catalog. It allows the Backstage instance to interface with Keycloak directly and perform automatic and recurring importing of assets. Once imported, these user and group entities can be used in the standard Backstage catalog model with the added benefits of Keycloak\u2019s diverse identity brokering capabilities.\\n\\n## MultiCluster Engine plugin for Backstage\\n\\nOne of the key focus areas for Backstage is the ability to provide a full, transparent service catalog to their developers. This includes mapping service dependencies on other components, resource ownership, and many more. Service dependencies should not include only the requirements of other services, but also model the underlying consumed resources. This plugin aims to provide a seamless, automated resource import for companies that use MulticlusterEngine from [Open Cluster Management](https://open-cluster-management.io/) or [Red Hat Advanced Cluster Management for Kubernetes](https://www.redhat.com/en/technologies/management/advanced-cluster-management) (RHACM) for their cluster fleet management. By connecting the Backstage instance to the Hub cluster, all managed clusters are discovered and imported into Backstage as standard catalog resources. In addition, the plugin also provides frontend components that fetch data from the hub cluster through the Kubernetes plugin on the Backstage instance as a proxy allowing users to quickly observe the current status of each of their clusters and providing quick access to the OpenShift console.\\n\\n## What\'s Next?\\n\\nWe\'ll be investigating a way to import the managed clusters into the Backstage Kubernetes plugin configuration. This capability will enable Backstage users the ability to observe workloads on the managed clusters and further simplify Backstage catalog maintenance and integration with Kubernetes cluster fleets."}]}')}}]);